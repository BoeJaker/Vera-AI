<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 900px;
                 background-color: #0f1419;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             

             

             
             /* position absolute is important and the container has to be relative or absolute as well. */
          div.popup {
                 position:absolute;
                 top:0px;
                 left:0px;
                 display:none;
                 background-color:#f5f4ed;
                 -moz-border-radius: 3px;
                 -webkit-border-radius: 3px;
                 border-radius: 3px;
                 border: 1px solid #808074;
                 box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.2);
          }

          /* hide the original tooltip */
          .vis-tooltip {
            display:none;
          }
             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"borderWidth": 1, "color": "#FFDFBA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:0", "label": "could you inspect your own s...", "mass": 1.2, "shape": "dot", "size": 19, "title": "{\n  \"properties\": {\n    \"topic\": \"plan\",\n    \"id\": \"mem_1755731776917\",\n    \"text\": \"could you inspect your own source code and suggest improvements\",\n    \"type\": \"Query\"\n  },\n  \"labels\": [\n    \"Query\"\n  ]\n}"}, {"borderWidth": 1, "color": "#BAFFFF", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:13", "label": "Session", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"conversation\",\n    \"id\": \"mem_1755731741659\",\n    \"text\": \"Session\",\n    \"type\": \"Session\"\n  },\n  \"labels\": [\n    \"Session\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFDFBA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:1", "label": "could you inspect your own s...", "mass": 1.2, "shape": "dot", "size": 19, "title": "{\n  \"properties\": {\n    \"topic\": \"plan\",\n    \"id\": \"mem_1755731581233\",\n    \"text\": \"could you inspect your own source code and suggest improvements\",\n    \"type\": \"Query\"\n  },\n  \"labels\": [\n    \"Query\"\n  ]\n}"}, {"borderWidth": 1, "color": "#BAFFFF", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:268", "label": "Session", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"conversation\",\n    \"id\": \"mem_1755731576099\",\n    \"text\": \"Session\",\n    \"type\": \"Session\"\n  },\n  \"labels\": [\n    \"Session\"\n  ]\n}"}, {"borderWidth": 1, "color": "#BAFFFF", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:2", "label": "Session", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"conversation\",\n    \"id\": \"mem_1756039243341\",\n    \"text\": \"Session\",\n    \"type\": \"Session\"\n  },\n  \"labels\": [\n    \"Session\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFFFBA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:6003", "label": "Orphaned Nodes Root", "mass": 2.6, "shape": "dot", "size": 30, "title": "{\n  \"properties\": {\n    \"name\": \"Orphaned Nodes Root\"\n  },\n  \"labels\": [\n    \"OrphanRoot\"\n  ]\n}"}, {"borderWidth": 1, "color": "#BAFFFF", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:3", "label": "Session", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"conversation\",\n    \"id\": \"mem_1756042792183\",\n    \"text\": \"Session\",\n    \"type\": \"Session\"\n  },\n  \"labels\": [\n    \"Session\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFCBA4", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:4", "label": "tool \nfocus code analysis  \n", "mass": 1.2, "shape": "dot", "size": 19, "title": "{\n  \"properties\": {\n    \"topic\": \"triage\",\n    \"id\": \"mem_1755731788637\",\n    \"text\": \"tool \\nfocus code analysis  \\n\",\n    \"type\": \"Response\"\n  },\n  \"labels\": [\n    \"Response\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFCBA4", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:5", "label": "tool \nfocus code analysis  \n", "mass": 1.2, "shape": "dot", "size": 19, "title": "{\n  \"properties\": {\n    \"topic\": \"triage\",\n    \"id\": \"mem_1755731602447\",\n    \"text\": \"tool \\nfocus code analysis  \\n\",\n    \"type\": \"Response\"\n  },\n  \"labels\": [\n    \"Response\"\n  ]\n}"}, {"borderWidth": 1, "color": "#BAFFFF", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:6", "label": "Session", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"conversation\",\n    \"id\": \"mem_1756044628882\",\n    \"text\": \"Session\",\n    \"type\": \"Session\"\n  },\n  \"labels\": [\n    \"Session\"\n  ]\n}"}, {"borderWidth": 1, "color": "#BAFFFF", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:7", "label": "Session", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"conversation\",\n    \"id\": \"mem_1756038979939\",\n    \"text\": \"Session\",\n    \"type\": \"Session\"\n  },\n  \"labels\": [\n    \"Session\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFB3BA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:8", "label": "[FocusManager] Focus set to:...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"focus\",\n    \"id\": \"mem_1755731788823\",\n    \"text\": \"[FocusManager] Focus set to: code analysis\",\n    \"type\": \"Thought\"\n  },\n  \"labels\": [\n    \"Thought\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFB3BA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:9", "label": "[FocusManager] Focus set to:...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"focus\",\n    \"id\": \"mem_1755731602645\",\n    \"text\": \"[FocusManager] Focus set to: code analysis\",\n    \"type\": \"Thought\"\n  },\n  \"labels\": [\n    \"Thought\"\n  ]\n}"}, {"borderWidth": 1, "color": "#BAFFFF", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:10", "label": "Session", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"conversation\",\n    \"id\": \"mem_1756039439506\",\n    \"text\": \"Session\",\n    \"type\": \"Session\"\n  },\n  \"labels\": [\n    \"Session\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFCBA4", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:11", "label": "Hello! \ud83d\udc4b\n\nHow can I help you...", "mass": 1.2, "shape": "dot", "size": 19, "title": "{\n  \"properties\": {\n    \"topic\": \"query\",\n    \"id\": \"mem_1756071018826\",\n    \"text\": \"Hello! \ud83d\udc4b\\n\\nHow can I help you today? \ud83d\ude0a\",\n    \"type\": \"Response\"\n  },\n  \"labels\": [\n    \"Response\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFDFBA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:239", "label": "Hello! \ud83d\udc4b\n\nHow can I help you...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"query\",\n    \"id\": \"mem_1756071017942\",\n    \"text\": \"Hello! \ud83d\udc4b\\n\\nHow can I help you today? \ud83d\ude0a\",\n    \"type\": \"Query\"\n  },\n  \"labels\": [\n    \"Query\"\n  ]\n}"}, {"borderWidth": 1, "color": "#BAFFFF", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:12", "label": "Session", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"conversation\",\n    \"id\": \"mem_1755731977699\",\n    \"text\": \"Session\",\n    \"type\": \"Session\"\n  },\n  \"labels\": [\n    \"Session\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFDFBA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:14", "label": "could you inspect your own s...", "mass": 1.2, "shape": "dot", "size": 19, "title": "{\n  \"properties\": {\n    \"topic\": \"plan\",\n    \"id\": \"mem_1755731980611\",\n    \"text\": \"could you inspect your own source code and suggest improvements\",\n    \"type\": \"Query\"\n  },\n  \"labels\": [\n    \"Query\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFCBA4", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:15", "label": "simple \n\nhello there!  how c...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"fast\",\n    \"id\": \"mem_1756070798914\",\n    \"text\": \"simple \\n\\nhello there!  how can I help you today?\",\n    \"type\": \"Triage\"\n  },\n  \"labels\": [\n    \"Triage\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFDFBA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:266", "label": "hello", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"query\",\n    \"id\": \"mem_1756070773063\",\n    \"text\": \"hello\",\n    \"type\": \"Query\"\n  },\n  \"labels\": [\n    \"Query\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFCBA4", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:16", "label": "tool \nfocus code analysis  \n", "mass": 1.2, "shape": "dot", "size": 19, "title": "{\n  \"properties\": {\n    \"topic\": \"triage\",\n    \"id\": \"mem_1755731992670\",\n    \"text\": \"tool \\nfocus code analysis  \\n\",\n    \"type\": \"Response\"\n  },\n  \"labels\": [\n    \"Response\"\n  ]\n}"}, {"borderWidth": 1, "color": "#BAFFFF", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:17", "label": "Session", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"conversation\",\n    \"id\": \"mem_1756039847530\",\n    \"text\": \"Session\",\n    \"type\": \"Session\"\n  },\n  \"labels\": [\n    \"Session\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFB3BA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:18", "label": "[FocusManager] Focus set to:...", "mass": 1.2, "shape": "dot", "size": 19, "title": "{\n  \"properties\": {\n    \"topic\": \"focus\",\n    \"id\": \"mem_1755731992823\",\n    \"text\": \"[FocusManager] Focus set to: code analysis\",\n    \"type\": \"Thought\"\n  },\n  \"labels\": [\n    \"Thought\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFDFBA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:19", "label": "inputchat_historyhistoryoutp...", "mass": 1.2, "shape": "dot", "size": 19, "title": "{\n  \"properties\": {\n    \"topic\": \"query\",\n    \"id\": \"mem_1755732328977\",\n    \"text\": \"inputchat_historyhistoryoutput\",\n    \"type\": \"Query\"\n  },\n  \"labels\": [\n    \"Query\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFCBA4", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:20", "label": "inputchat_historyhistoryoutp...", "mass": 1.2, "shape": "dot", "size": 19, "title": "{\n  \"properties\": {\n    \"topic\": \"query\",\n    \"id\": \"mem_1755732329920\",\n    \"text\": \"inputchat_historyhistoryoutput\",\n    \"type\": \"Response\"\n  },\n  \"labels\": [\n    \"Response\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFDFBA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:21", "label": "using toolchains, could you ...", "mass": 1.2, "shape": "dot", "size": 19, "title": "{\n  \"properties\": {\n    \"topic\": \"plan\",\n    \"id\": \"mem_1755734118397\",\n    \"text\": \"using toolchains, could you inspect your own source code and suggest improvements. please create a comprehensive improvement plan with actionables\",\n    \"type\": \"Query\"\n  },\n  \"labels\": [\n    \"Query\"\n  ]\n}"}, {"borderWidth": 1, "color": "#BAFFFF", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:22", "label": "Session", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"conversation\",\n    \"id\": \"mem_1756059035365\",\n    \"text\": \"Session\",\n    \"type\": \"Session\"\n  },\n  \"labels\": [\n    \"Session\"\n  ]\n}"}, {"borderWidth": 1, "color": "#D4FFBA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:23", "label": "Node 4:0658720f-ad9e-48e1-a1...", "mass": 2.0, "shape": "dot", "size": 26, "title": "{\n  \"properties\": {\n    \"id\": \"proj_alpha\",\n    \"type\": \"project\",\n    \"priority\": \"high\",\n    \"status\": \"active\"\n  },\n  \"labels\": [\n    \"Project\"\n  ]\n}"}, {"borderWidth": 1, "color": "#D4FFBA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:1336", "label": "Decision: use exponential ba...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"decision\",\n    \"id\": \"mem_1759091455814\",\n    \"text\": \"Decision: use exponential backoff with jitter.\",\n    \"type\": \"Answer\"\n  },\n  \"labels\": [\n    \"Answer\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFDFBA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:1335", "label": "Investigate API rate limits ...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"risk\",\n    \"id\": \"mem_1759091453444\",\n    \"text\": \"Investigate API rate limits for upstream service.\",\n    \"type\": \"Query\"\n  },\n  \"labels\": [\n    \"Query\"\n  ]\n}"}, {"borderWidth": 1, "color": "#D4FFBA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:1326", "label": "Decision: use exponential ba...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"decision\",\n    \"id\": \"mem_1759086480425\",\n    \"text\": \"Decision: use exponential backoff with jitter.\",\n    \"type\": \"Answer\"\n  },\n  \"labels\": [\n    \"Answer\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFDFBA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:1325", "label": "Investigate API rate limits ...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"risk\",\n    \"id\": \"mem_1759086480092\",\n    \"text\": \"Investigate API rate limits for upstream service.\",\n    \"type\": \"Query\"\n  },\n  \"labels\": [\n    \"Query\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFB3BA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:79", "label": "Decision: use exponential ba...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"decision\",\n    \"id\": \"mem_1755556956205\",\n    \"text\": \"Decision: use exponential backoff with jitter.\",\n    \"type\": \"Answer\"\n  },\n  \"labels\": [\n    \"Thought\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFB3BA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:78", "label": "Investigate API rate limits ...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"risk\",\n    \"id\": \"mem_1755556955671\",\n    \"text\": \"Investigate API rate limits for upstream service.\",\n    \"type\": \"Query\"\n  },\n  \"labels\": [\n    \"Thought\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFB3BA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:47", "label": "Decision: use exponential ba...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"decision\",\n    \"id\": \"mem_1755548733212\",\n    \"text\": \"Decision: use exponential backoff with jitter.\",\n    \"type\": \"thought\"\n  },\n  \"labels\": [\n    \"Thought\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFB3BA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:46", "label": "Investigate API rate limits ...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"risk\",\n    \"id\": \"mem_1755548732308\",\n    \"text\": \"Investigate API rate limits for upstream service.\",\n    \"type\": \"thought\"\n  },\n  \"labels\": [\n    \"Thought\"\n  ]\n}"}, {"borderWidth": 1, "color": "#E6BAFF", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:25", "label": "Node 4:0658720f-ad9e-48e1-a1...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"id\": \"doc_spec_v1\",\n    \"doc_type\": \"spec\",\n    \"type\": \"document\",\n    \"entity_id\": \"proj_alpha\"\n  },\n  \"labels\": [\n    \"Document\"\n  ]\n}"}, {"borderWidth": 1, "color": "#E6BAFF", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:24", "label": "Node 4:0658720f-ad9e-48e1-a1...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"repo\": \"git@example.com/lib_net.git\",\n    \"id\": \"lib_net\",\n    \"type\": \"codebase\"\n  },\n  \"labels\": [\n    \"Codebase\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFCBA4", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:26", "label": "toolchain \nfocus: self-impro...", "mass": 1.2, "shape": "dot", "size": 19, "title": "{\n  \"properties\": {\n    \"topic\": \"triage\",\n    \"id\": \"mem_1755734139416\",\n    \"text\": \"toolchain \\nfocus: self-improvement  \\n\",\n    \"type\": \"Response\"\n  },\n  \"labels\": [\n    \"Response\"\n  ]\n}"}, {"borderWidth": 1, "color": "#BAFFFF", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:27", "label": "Session", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"conversation\",\n    \"id\": \"mem_1756039970555\",\n    \"text\": \"Session\",\n    \"type\": \"Session\"\n  },\n  \"labels\": [\n    \"Session\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFB3BA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:28", "label": "Investigate API rate limits ...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"risk\",\n    \"id\": \"mem_1755547855115\",\n    \"text\": \"Investigate API rate limits for upstream service.\",\n    \"type\": \"thought\"\n  },\n  \"labels\": [\n    \"Thought\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFB3BA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:29", "label": "[FocusManager] Focus set to:...", "mass": 1.2, "shape": "dot", "size": 19, "title": "{\n  \"properties\": {\n    \"topic\": \"focus\",\n    \"id\": \"mem_1755734139603\",\n    \"text\": \"[FocusManager] Focus set to: : self-improvement\",\n    \"type\": \"Thought\"\n  },\n  \"labels\": [\n    \"Thought\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFB3BA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:30", "label": "Investigate API rate limits ...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"risk\",\n    \"id\": \"mem_1755547891934\",\n    \"text\": \"Investigate API rate limits for upstream service.\",\n    \"type\": \"thought\"\n  },\n  \"labels\": [\n    \"Thought\"\n  ]\n}"}, {"borderWidth": 1, "color": "#E6BAFF", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:31", "label": "[{\"tool\": \"inspect system so...", "mass": 1.2, "shape": "dot", "size": 19, "title": "{\n  \"properties\": {\n    \"topic\": \"plan\",\n    \"id\": \"mem_1755734288753\",\n    \"text\": \"[{\\\"tool\\\": \\\"inspect system source code\\\", \\\"input\\\": \\\"Please provide all available source code.\\\"}, {\\\"tool\\\": \\\"Query Fast LLM\\\", \\\"input\\\": \\\"I have received the source code from the \u0027inspect system source code\u0027 tool. Please analyze the code for potential improvements in terms of code style, efficiency, error handling, security, and overall maintainability. Focus on identifying areas where refactoring could lead to a more robust and scalable system. Provide a comprehensive improvement plan with actionable steps, categorized by area of improvement (e.g., Code Style, Efficiency, Error Handling, Security, Maintainability).  Each actionable step should include a brief description of the change and the rationale behind it. Prioritize improvements based on their potential impact and feasibility.\\\"}]\",\n    \"type\": \"Plan\"\n  },\n  \"labels\": [\n    \"Plan\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFB3BA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:32", "label": "Investigate API rate limits ...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"risk\",\n    \"id\": \"mem_1755547922183\",\n    \"text\": \"Investigate API rate limits for upstream service.\",\n    \"type\": \"thought\"\n  },\n  \"labels\": [\n    \"Thought\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFDFBA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:33", "label": "can you execute whoami", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"plan\",\n    \"id\": \"mem_1756071035375\",\n    \"text\": \"can you execute whoami\",\n    \"type\": \"Query\"\n  },\n  \"labels\": [\n    \"Query\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFB3BA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:34", "label": "Investigate API rate limits ...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"risk\",\n    \"id\": \"mem_1755547951962\",\n    \"text\": \"Investigate API rate limits for upstream service.\",\n    \"type\": \"thought\"\n  },\n  \"labels\": [\n    \"Thought\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFB3BA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:35", "label": "Decision: use exponential ba...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"decision\",\n    \"id\": \"mem_1755547952721\",\n    \"text\": \"Decision: use exponential backoff with jitter.\",\n    \"type\": \"thought\"\n  },\n  \"labels\": [\n    \"Thought\"\n  ]\n}"}, {"borderWidth": 1, "color": "#E6BAFF", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:36", "label": "Step 1 - inspect system sour...", "mass": 1.2, "shape": "dot", "size": 19, "title": "{\n  \"properties\": {\n    \"topic\": \"toolchain\",\n    \"id\": \"mem_1755734388273\",\n    \"text\": \"Step 1 - inspect system source code: # Vera.py - Vera - Sans corps - Sine corpore - \u0431\u0435\u0437 \u0442\u0435\u043b\u0430 - \u4f53\u306a\u3057\u3067 \\n\\n\\\"\\\"\\\"\\nVera - AI System\\nToolchain, multi-agent system with proactive focus management and tool execution.\\n\\n\\\"\\\"\\\"\\n \\n# --- Imports ---\\nimport sys\\nimport os\\nimport subprocess\\nimport json\\nfrom typing import List, Dict, Any, Type\\nfrom typing import Optional, Callable\\nimport threading\\nimport time\\n# import asyncio\\nimport inspect\\nimport psutil\\nimport re\\nfrom pydantic import BaseModel, Field\\nimport io\\nimport traceback\\nimport ollama\\n# import types\\nfrom collections.abc import Iterator\\nfrom urllib.parse import quote_plus\\n\\nfrom duckduckgo_search import DDGS\\nfrom langchain_core.tools import tool\\nfrom langchain_community.llms import Ollama\\nfrom langchain.agents import initialize_agent, Tool, AgentType\\nfrom langchain.memory import ConversationBufferMemory, VectorStoreRetrieverMemory, CombinedMemory\\nfrom langchain.vectorstores import Chroma\\nfrom langchain_community.embeddings import OllamaEmbeddings\\nfrom langchain_community.agent_toolkits import PlayWrightBrowserToolkit\\nfrom langchain_community.tools.playwright.utils import (\\n    create_sync_playwright_browser,\\n    create_async_playwright_browser,\\n)\\nfrom langchain.tools import BaseTool\\n# --- Local Imports ---\\nfrom executive_0_9 import executive\\nfrom memory import *\\n# from models import fast_llm, deep_llm\\n# from memory import init_memory\\n# from speech import listen, speak\\n# from executor import tools\\n# from executive \\n\\nclass PythonInput(BaseModel):\\n    code: str = Field(..., description=\\\"Python code to execute\\\")\\n\\n\\nclass UnrestrictedPythonTool(BaseTool):\\n    name: str = \\\"unrestricted_python\\\"\\n    description: str = \\\"Executes arbitrary Python code. Full access to Python runtime. For advanced tasks.\\\"\\n    args_schema: Type[BaseModel] = PythonInput\\n\\n    def _run(self, code: str) -\u003e str:\\n        old_stdout = sys.stdout\\n        redirected_output = sys.stdout = io.StringIO()\\n\\n        local_vars = {}\\n\\n        try:\\n            try:\\n                result = eval(code, globals(), local_vars)\\n                if result is not None:\\n                    print(result)\\n            except SyntaxError:\\n                exec(code, globals(), local_vars)\\n\\n            output = redirected_output.getvalue()\\n            return output.strip() or \\\"[No output]\\\"\\n\\n        except Exception:\\n            return f\\\"[Error]\\\\n{traceback.format_exc()}\\\"\\n\\n        finally:\\n            sys.stdout = old_stdout\\n\\n    def _arun(self, code: str):\\n        raise NotImplementedError(\\\"Async execution not supported.\\\")\\n\\n@tool\\ndef run_python(code: str) -\u003e str:\\n    \\\"\\\"\\\"Run arbitrary Python code and return its output.\\\"\\\"\\\"\\n    old_stdout = sys.stdout\\n    redirected_output = sys.stdout = io.StringIO()\\n    local_vars = {}\\n\\n    try:\\n        try:\\n            result = eval(code, globals(), local_vars)\\n            if result is not None:\\n                print(result)\\n        except SyntaxError:\\n            exec(code, globals(), local_vars)\\n\\n        output = redirected_output.getvalue()\\n        return output.strip() or \\\"[No output]\\\"\\n\\n    except Exception:\\n        return f\\\"[Error]\\\\n{traceback.format_exc()}\\\"\\n\\n    finally:\\n        sys.stdout = old_stdout\\n\\nCONFIG_FILE = \\\"vera_models.json\\\"\\n\\ndef choose_models_from_installed():\\n    # Get list of installed Ollama models\\n    available_models = [m[\\\"model\\\"] for m in ollama.list()[\\\"models\\\"]]\\n    if not available_models:\\n        raise RuntimeError(\\\"[Vera Model Loader] No Ollama models found! Please install some models first.\\\")\\n\\n    # Default models (used if config file doesn\u0027t exist)\\n    default_models = {\\n        \\\"embedding_model\\\": \\\"mistral:7b\\\",\\n        \\\"fast_llm\\\": \\\"gemma2\\\",\\n        \\\"intermediate_llm\\\": \\\"gemma3:12b\\\",\\n        \\\"deep_llm\\\": \\\"gemma3:27b\\\",\\n        \\\"reasoning_llm\\\": \\\"gpt-oss:20b\\\",\\n        \\\"tool_llm\\\": \\\"gemma2\\\"\\n    }\\n\\n    # Load last used models if available\\n    if os.path.exists(CONFIG_FILE):\\n        try:\\n            with open(CONFIG_FILE, \\\"r\\\") as f:\\n                saved_models = json.load(f)\\n                default_models.update(saved_models)\\n        except Exception:\\n            print(\\\"[Vera Model Loader] Warning: could not read config file, using defaults.\\\")\\n\\n    chosen_models = {}\\n    print(\\\"\\\\n[Vera Model Loader] Select a model for each category:\\\")\\n    for key, default in default_models.items():\\n        print(f\\\"\\\\nSelect model for {key.replace(\u0027_\u0027, \u0027 \u0027).title()} (default: {default})\\\")\\n        for idx, model in enumerate(available_models, 1):\\n            print(f\\\"{idx}. {model}\\\")\\n        choice = input(f\\\"Enter choice (1-{len(available_models)}) or press Enter for default: \\\").strip()\\n\\n        if choice.isdigit() and 1 \u003c= int(choice) \u003c= len(available_models):\\n            chosen_models[key] = available_models[int(choice) - 1]\\n        else:\\n            chosen_models[key] = default\\n\\n    # Save chosen models for next run\\n    try:\\n        with open(CONFIG_FILE, \\\"w\\\") as f:\\n            json.dump(chosen_models, f, indent=2)\\n    except Exception as e:\\n        print(f\\\"[Vera Model Loader] Warning: could not save config: {e}\\\")\\n\\n    print(\\\"\\\\n[Vera Model Loader] Selected Models:\\\")\\n    for key, model in chosen_models.items():\\n        print(f\\\"{key}: {model}\\\")\\n    print()\\n\\n    return chosen_models\\n\\n# --- Agent Class ---\\nclass Vera:\\n    \\\"\\\"\\\"Vera class that manages multiple LLMs and tools for complex tasks.\\n    Started off as a triple agent, but now it has grown into a multi-agent system.\\\"\\\"\\\"\\n\\n    def __init__(self, chroma_path=\\\"./vera_agent_memory\\\"):\\n        selected_models = choose_models_from_installed()\\n        self.embedding_llm = model=selected_models[\\\"embedding_model\\\"]\\n        self.fast_llm = Ollama(model=selected_models[\\\"fast_llm\\\"], temperature=0.2)\\n        self.intermediate_llm = Ollama(model=selected_models[\\\"intermediate_llm\\\"], temperature=0.4)\\n        self.deep_llm = Ollama(model=selected_models[\\\"deep_llm\\\"], temperature=0.6)\\n        self.reasoning_llm = Ollama(model=selected_models[\\\"reasoning_llm\\\"], temperature=0.7)\\n        self.tool_llm = Ollama(model=selected_models[\\\"tool_llm\\\"], temperature=0)\\n        \\n        # --- Setup Memory ---\\n        NEO4J_URI = os.getenv(\\\"NEO4J_URI\\\", \\\"bolt://localhost:7687\\\")\\n        NEO4J_USER = os.getenv(\\\"NEO4J_USER\\\", \\\"neo4j\\\")\\n        NEO4J_PASSWORD = os.getenv(\\\"NEO4J_PASSWORD\\\", \\\"testpassword\\\")\\n        CHROMA_DIR = os.getenv(\\\"CHROMA_DIR\\\", \\\"./chroma_store\\\")\\n        ARCHIVE_PATH = os.getenv(\\\"ARCHIVE_JSONL\\\", \\\"./archive/memory_archive.jsonl\\\")\\n\\n        self.mem = HybridMemory(\\n            neo4j_uri=NEO4J_URI,\\n            neo4j_user=NEO4J_USER,\\n            neo4j_password=NEO4J_PASSWORD,\\n            chroma_dir=CHROMA_DIR,\\n            archive_jsonl=ARCHIVE_PATH,\\n    )\\n        # Start a session (Tier 2)\\n        print(\\\"Starting a session...\\\")\\n        self.sess = self.mem.start_session(metadata={\\\"agent\\\": \\\"vera\\\"})\\n        self.mem.add_session_memory(self.sess.id, \\\"Session\\\", \\\"Session\\\", metadata={\\\"topic\\\": \\\"conversation\\\"})\\n        # mem.link_session_focus(sess.id, [\\\"proj_alpha\\\"])  # Session focuses on the project\\n        # Add session thoughts and optionally promote one\\n        # print(\\\"Adding session thoughts...\\\")\\n        # t1 = mem.add_session_memory(sess.id, \\\"Investigate API rate limits for upstream service.\\\", {\\\"topic\\\": \\\"risk\\\"})\\n        # t2 = mem.add_session_memory(sess.id, \\\"Decision: use exponential backoff with jitter.\\\", {\\\"topic\\\": \\\"decision\\\"}, promote=True)\\n\\n \\n\\n        # --- Shared ChromaDB Memory ---\\n        embeddings = OllamaEmbeddings(model=self.embedding_llm)\\n        self.vectorstore = Chroma(\\n            persist_directory=chroma_path,\\n            embedding_function=embeddings\\n        )\\n\\n        # Vector memory (long-term)\\n        self.vector_memory = VectorStoreRetrieverMemory(\\n            retriever=self.vectorstore.as_retriever(search_kwargs={\\\"k\\\": 5})\\n        )\\n\\n        # Short-term conversation memory (buffer)\\n        self.buffer_memory = ConversationBufferMemory(\\n            memory_key=\\\"chat_history\\\",  # Where chat history is stored\\n            input_key=\\\"input\\\",     # Explicitly say which is the actual input\\n            return_messages=True\\n        )\\n        # Plan memory (short-term, for storing plans in buffer)\\n        self.plan_memory = ConversationBufferMemory(\\n            memory_key=\\\"plan_history\\\",\\n            input_key=\\\"input\\\",\\n            return_messages=True\\n        )\\n        # Plan long-term memory (vector store for plans)\\n        self.plan_vectorstore = Chroma(\\n            persist_directory=os.path.join(chroma_path, \\\"plans\\\"),\\n            embedding_function=embeddings\\n        )\\n        self.plan_vector_memory = VectorStoreRetrieverMemory(\\n            retriever=self.plan_vectorstore.as_retriever(search_kwargs={\\\"k\\\": 5})\\n        )\\n        # --- Proactive Focus Manager ---\\n        self.focus_manager = ProactiveFocusManager(agent=self)\\n        self.triage_memory = False\\n        # Combined memory = short-term + long-term\\n        self.memory = CombinedMemory(memories=[self.buffer_memory, self.vector_memory])\\n        # --- Fast Agent (small Gemma for quick replies) ---\\n        self.fast_llm = Ollama(model=\\\"gemma2\\\", temperature=0.2)\\n        # --- Intermediate Agent (medium Gemma for more complex tasks) ---\\n        self.intermediate_llm = Ollama(model=\\\"gemma3:12b\\\", temperature=0.4)\\n        # --- Deep Agent (larger Gemma for reasoning) ---\\n        self.deep_llm = Ollama(model=\\\"gemma3:27b\\\", temperature=0.6)\\n        # --- Reasoning Agent --- (Reasoning-heavy tasks)\\n        self.reasoning_llm = Ollama(model=\\\"gpt-oss:20b\\\", temperature=0.7)\\n        # --- Tool Executor ---\\n        self.tool_llm = Ollama(model=\\\"gemma2\\\", temperature=0)\\n        \\n        # Playwright browser setup\\n        self.sync_browser = create_sync_playwright_browser()\\n        self.toolkit = PlayWrightBrowserToolkit.from_browser(sync_browser=self.sync_browser)\\n        self.playwright_tools = self.toolkit.get_tools()\\n        print(f\\\"[Vera] Loaded {self.playwright_tools} Playwright tools.\\\")\\n        \\n        self.executive_instance = executive(vera_instance=self)\\n\\n        # Tool setup\\n        self.tools = self.load_tools() + self.playwright_tools\\n        print(f\\\"[Vera] Loaded {self.tools} tools.\\\")\\n        # Fast Agent that can handle simple tool queries\\n        self.light_agent = initialize_agent(\\n            self.tools,\\n            self.tool_llm,\\n            agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\\n            memory=self.memory,\\n            verbose=True\\n        )\\n        # Deep Agent that can call tools itself\\n        self.deep_agent = initialize_agent(\\n            self.tools,\\n            self.deep_llm,\\n            agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\\n            memory=self.memory,\\n            verbose=True\\n        )\\n        self.toolchain = ToolChainPlanner(self, self.tools)\\n\\n\\n        \\n        # Define callback to handle proactive thoughts:\\n        def handle_proactive(thought):\\n            print(f\\\"Proactive Thought: {thought}\\\")\\n            # Here, you could also feed it to fast_llm or notify the user interface\\n            # e.g., self.fast_llm.predict(thought) or queue message\\n\\n        # Set callback\\n        self.focus_manager.proactive_callback = handle_proactive\\n\\n    def review_output(self, query, response):\\n        review_prompt = f\\\"\\\"\\\"\\n            You are a reviewer.\\n            Here is the original query: {query}\\n            Here is the response: {response}\\n\\n            Decide:\\n            - Is this response correct and complete? (yes/no)\\n            - If no, explain briefly what is missing or wrong.\\n\\n            Output \u0027YES\u0027 if correct, or \u0027NO: \u003cbrief reason\u003e\u0027 if not.\\n            \\\"\\\"\\\"\\n        review = self.fast_llm.invoke(review_prompt)\\n        return review.strip()\\n    \\n    def read_own_source(self) -\u003e str:\\n        \\\"\\\"\\\"\\n        Reads and returns the full Python source code of the file this function is called from.\\n        \\\"\\\"\\\"\\n        try:\\n            # Get the path of the current running script (the file this function is defined in)\\n            current_file = inspect.getfile(inspect.currentframe())\\n            with open(current_file, \u0027r\u0027, encoding=\u0027utf-8\u0027) as f:\\n                source_code = f.read()\\n            return source_code\\n        except Exception as e:\\n            return f\\\"Error reading source code: {e}\\\"\\n        \\n    def fast_llm_func(self, q):\\n        result=\\\"\\\"\\n        for x in self.stream_llm_with_memory(self.fast_llm, q, long_term=False, short_term=True):\\n            text = x if isinstance(x, str) else str(x)\\n            # print(r)\\n            result += x\\n            yield x\\n        self.mem.add_session_memory(self.sess.id, text, \\\"Answer\\\", {\\\"topic\\\": \\\"decision\\\", \\\"agent\\\": self.fast_llm})\\n        # return result\\n\\n    def deep_llm_func(self, q):\\n        result=\\\"\\\"\\n        for x in self.stream_llm_with_memory(self.deep_llm, q, long_term=True, short_term=True):\\n            text = x if isinstance(x, str) else str(x)\\n            # print(r)\\n            result += x\\n            yield x\\n        self.mem.add_session_memory(self.sess.id, text, \\\"Answer\\\", {\\\"topic\\\": \\\"decision\\\", \\\"agent\\\": self.deep_llm})\\n        # return result\\n    \\n    def write_file_tool(self, q):\\n        try:\\n            # Expecting \\\"path::content\\\"\\n            path, content = str(q).split(\\\"|||\\\", 1)\\n            with open(path.strip(), \u0027w\u0027, encoding=\u0027utf-8\u0027) as f:\\n                f.write(content)\\n            self.mem.attach_document(path.strip(), os.path.basename(path.strip()), content, {\\\"doc_type\\\": \\\"generated\\\"})\\n            return f\\\"File written successfully to {path}\\\"\\n        except ValueError:\\n            return \\\"Invalid write_to_file input format. Use: path|||content\\\"\\n        except Exception as e:\\n            return f\\\"Error writing file: {e}\\\"\\n\\n    def read_file(self, q):\\n        \\\"\\\"\\\"Read a file and return its contents.\\\"\\\"\\\"\\n        try:\\n            if os.path.exists(q):\\n                with open(q, \u0027r\u0027, encoding=\u0027utf-8\u0027) as f:\\n                    content = f.read()\\n                    self.mem.attach_document(q.strip(), os.path.basename(q.strip()), content, {\\\"doc_type\\\": \\\"file\\\"})\\n                    return content\\n        except Exception as e:\\n            return f\\\"Error reading file {q}: {e}\\\"\\n    \\n    def run_command_stream(self, q):\\n        # Start the process\\n        process = subprocess.Popen(\\n            q,\\n            shell=True,\\n            stdout=subprocess.PIPE,\\n            stderr=subprocess.STDOUT,\\n            text=True,\\n            bufsize=1  # line-buffered\\n        )\\n\\n        # Yield lines as they appear\\n        for line in process.stdout:\\n            yield line.rstrip()  # remove trailing newline if desired\\n        self.mem.add_session_memory(self.sess.id, f\\\"Executed command: {q}\\\", \\\"Command\\\", {\\\"topic\\\": \\\"bash\\\"})\\n        process.stdout.close()\\n        return_code = process.wait()\\n        if return_code:\\n            raise subprocess.CalledProcessError(return_code, q)\\n\\n\\n    def load_tools(self):\\n        return [\\n            # Tool(\\n            # name=\\\"Add Google Calendar Event\\\",\\n            # func=lambda q: self.add_event_google(q),\\n            # description=\\\"Add an event to Google Calendar.\\\"\\n            # ),\\n            # Tool(\\n            # name=\\\"List Projects\\\",\\n            # func=lambda q: self.list_projects(),\\n            # description=\\\"List available projects.\\\"\\n            # ),\\n            Tool( \\n            name=\\\"Query Fast LLM\\\",\\n            func=self.fast_llm_func,\\n            description=\\\"capable of creative writing, reviewing text, summarizing, combining text, improving text. Fast but can be inaccurate\\\"\\n            #\\\"Given a query and context, reviews or summarizes the response for clarity and brevity. Acts as a quick reviewer, extractor, transformer or summarizer, not a solution provider.\\\"\\n           ),\\n            Tool( \\n            name=\\\"Query Deep LLM\\\",\\n            func=self.deep_llm_func,\\n            description=\\\"capable of creative writing, reviewing text, summarizing, combining text, improving text. slow and accurate\\\"\\n            #\\\"Given a query and context, reviews, improves or summarizes the response. Acts as a detailed reviewer, extractor, transformer  or summarizer, not a solution provider.\\\"\\n            ),\\n            Tool(\\n            name=\\\"Bash Shell\\\",\\n            # func=lambda q: subprocess.check_output(q, shell=True, text=True, stderr=__import__(\u0027subprocess\u0027).STDOUT), # REMOVE inline import\\n            func=lambda q: self.run_command_stream(q),\\n            description=\\\"Execute a bash shell command or script, and return its output.\\\"\\n            ),\\n            Tool(\\n            name=\\\"Run Python Code\\\",\\n            func=lambda q: run_python(q),\\n            description=\\\"Execute a Python code snippet.\\\"\\n            ),\\n            Tool(\\n            name=\\\"Read File\\\",\\n            func=lambda q: open(q, \u0027r\u0027).read() if os.path.exists(q) else f\\\"File {q} does not exist.\\\",\\n            description=\\\"Read the contents of a file. Provide the full path to the file.\\\"\\n            ),\\n            Tool(\\n            name=\\\"Write File\\\",\\n            func=self.write_file_tool,\\n            description=\\\"Given a filepath and content, saves content to a file. Input format: filepath, followed by \u0027|||\u0027 delimiter, then the file content. Example input: /path/to/file.txt|||This is the file content. Do NOT use newlines as delimiter.\\\"\\n            ),\\n            Tool(\\n            name=\\\"List Python Modules\\\",\\n            func=lambda q: sorted(list(sys.modules.keys())),\\n            description=\\\"List all currently loaded Python modules.\\\"\\n            ),\\n            Tool(\\n                name=\\\"List Installed Programs\\\",\\n                func=lambda q: subprocess.check_output(\\n                    \\\"wmic product get name\\\" if sys.platform == \\\"win32\\\" else \\\"dpkg --get-selections\\\" if sys.platform.startswith(\\\"linux\\\") else \\\"brew list\\\",\\n                    shell=True,\\n                    text=True,\\n                    stderr=subprocess.STDOUT\\n                ),\\n                description=\\\"List all installed programs on the system.\\\"\\n            ),\\n            # Tool(\\n            #     name=\\\"Review Output\\\",\\n            #     func=lambda q: self.review_output(q.get(\\\"query\\\", \\\"\\\"), q.get(\\\"response\\\", \\\"\\\")) if isinstance(q, dict) and \\\"query\\\" in q and \\\"response\\\" in q else \\\"Input must be a dict with \u0027query\u0027 and \u0027response\u0027 keys.\\\",\\n            #     description=\\\"Review an output given the original query and response. Input should be a dict: {\u0027query\u0027: \u003cquery\u003e, \u0027response\u0027: \u003cresponse\u003e}.\\\"\\n            # ),\\n            Tool(\\n                name=\\\"Search Memory\\\",\\n                func=lambda q: \\\"\\\\n\\\".join(\\n                    [doc.page_content for doc in self.vectorstore.similarity_search(q, k=5)]\\n                ),\\n                description=\\\"Searches the agent\u0027s long-term memory for relevant information given a query.\\\"\\n            ),\\n            Tool(\\n                name=\\\"DuckDuckGo Web Search\\\",\\n                func=lambda q: self.duckduckgo_search(q),\\n                description=\\\"Search the web using DuckDuckGo and return the top results.\\\"\\n            ),\\n            Tool(\\n                name=\\\"inspect system source code\\\",\\n                func=lambda q: self.read_own_source(),\\n                description=\\\"Allows you to peer at your own code. helpful for understanding your own inner workings. not used for most tasks.\\\"\\n            ),\\n            Tool(\\n            name=\\\"Scheduling Assistant\\\",\\n            func=self.executive_instance.main,\\n            description=\\\"Run the executive scheduling assistant with a query. It has access to the users calendars todo lists and scheduling apps, It can plan and execute scheduling and time management tasks, manage events, calendars, and more. Input should be a query string.\\\"\\n        )\\n            # UnrestrictedPythonTool\\n        ]\\n\\n    def duckduckgo_search(self, query: str, max_results: int = 5) -\u003e str:\\n        \\\"\\\"\\\"Search the web using DuckDuckGo and return the top results as a string of titles, urls, and short descriptions. Further processing may be required to extract useful information.\\\"\\\"\\\"\\n        \\n        query_encoded = quote_plus(query)\\n        \\n        try:\\n            with DDGS() as ddgs:\\n                results = ddgs.text(query, max_results=max_results)\\n                output = []\\n                for idx, r in enumerate(results, 1):\\n                    title = r.get(\\\"title\\\", \\\"\\\")\\n                    href = r.get(\\\"href\\\", \\\"\\\")\\n                    body = r.get(\\\"body\\\", \\\"\\\")\\n                    output.append(f\\\"{idx}. {title}\\\\n{href}\\\\n{body}\\\\n\\\")\\n                return \\\"\\\\n\\\".join(output) if output else \\\"No results found.\\\"\\n        except Exception as e:\\n            return f\\\"[DuckDuckGo Search Error] {e}\\\"\\n\\n    # --- Tool Implementations ---\\n    def add_event_google(self, q):\\n        return f\\\"Google event created: {q}\\\"\\n\\n    def list_projects(self):\\n        return [\\\"Project A\\\", \\\"Project B\\\"]\\n\\n    # --- Streaming wrapper ---\\n    def stream_llm(self, llm, prompt):\\n        sys.stdout.write(\\\"\\\\n\\\")\\n        sys.stdout.flush()\\n        output = []\\n        for chunk in llm.stream(prompt):\\n            sys.stdout.write(chunk)\\n            sys.stdout.flush()\\n            output.append(chunk)\\n            yield(chunk)\\n        print()  # final newline\\n        return \\\"\\\".join(output)\\n    \\n    import requests\\n\\n    def stream_ollama_raw(model, prompt):\\n        url = \\\"http://localhost:11434/api/generate\\\"\\n        payload = {\\n            \\\"model\\\": model,\\n            \\\"prompt\\\": prompt,\\n            \\\"stream\\\": True\\n        }\\n        with requests.post(url, json=payload, stream=True) as r:\\n            for line in r.iter_lines():\\n                if line:\\n                    data = line.decode(\\\"utf-8\\\")\\n                    print(data)  # raw JSON string\\n\\n    \\n    # --- Streaming wrapper with memory injection ---\\n    def stream_llm_with_memory(self, llm, user_input, extra_context=None, long_term=True, short_term=True):\\n        # Retrieve relevant memory context\\n        past_context = \\\"\\\"\\n        relevant_history = \\\"\\\"\\n        if short_term:\\n            past_context = self.memory.load_memory_variables({\\\"input\\\": user_input}).get(\\\"chat_history\\\", \\\"\\\")\\n           \\n        if long_term:\\n            relevant_history = self.vector_memory.load_memory_variables({\\\"input\\\": user_input})\\n          \\n        # Combine past context + any extra agent reasoning\\n        full_prompt = f\\\"Conversation so far:\\\\n{str(past_context)}\\\"\\n        full_prompt += f\\\"Relevant conversation history{relevant_history}\\\"\\n\\n        print(f\\\"Memory:\\\\n\\\\n{full_prompt}\\\\n\\\\n\\\")\\n\\n        if extra_context:\\n            full_prompt += f\\\"\\\\n\\\\nExtra reasoning/context from another agent:\\\\n{extra_context}\\\"\\n        full_prompt += f\\\"\\\\n\\\\nUser: {user_input}\\\\nAssistant:\\\"\\n\\n        full_prompt += f\\\"\\\\n\\\\nTools available via the agent system:{[tool.name for tool in self.tools]}\\\\n\\\"\\n        # print(full_prompt)\\n        sys.stdout.write(\\\"\\\\n\\\")\\n        sys.stdout.flush()\\n        output = []\\n        for chunk in llm.stream(full_prompt):\\n            sys.stdout.write(chunk)\\n            sys.stdout.flush()\\n            output.append(chunk)\\n            yield(chunk)\\n        print()\\n\\n        # Save both question and answer to shared memory\\n        ai_output = \\\"\\\".join(output)\\n        self.memory.save_context({\\\"input\\\": user_input}, {\\\"output\\\": ai_output})\\n        self.vectorstore.persist()\\n        self.focus_manager.update_latest_conversation(f\\\"User Query: {user_input}\\\")\\n        self.focus_manager.update_latest_conversation(f\\\"Agent Response: {user_input}\\\")\\n\\n        return ai_output\\n\\n    # --- Save memory entry ---\\n    def save_to_memory(self, user_input, llm_output):\\n        \\\"\\\"\\\"Save both user query and LLM reply to shared memory.\\\"\\\"\\\"\\n        self.buffer_memory.chat_memory.add_user_message(user_input)\\n        self.buffer_memory.chat_memory.add_ai_message(llm_output)\\n        self.vector_memory.save_context({\\\"input\\\": user_input}, {\\\"output\\\": llm_output})\\n        self.vectorstore.persist()\\n        # self.mem.add_session_memory(self.sess.id, f\\\"input: {user_input}\\\\noutput: {llm_output}\\\", \\\"Thought\\\", {\\\"topic\\\": \\\"decision\\\"}, promote=True)\\n\\n\\n    def execute_tool_chain(self, query):\\n        \\\"\\\"\\\"Plan and execute multiple tools in sequence, replacing inputs based on previous outputs.\\\"\\\"\\\"\\n\\n        planning_prompt = f\\\"\\\"\\\"\\n            You are a planning assistant.\\n            Available tools: {[(tool.name, tool.description) for tool in self.tools]}.\\n            The query is: {query}\\n\\n            Plan a sequence of tool calls to solve the request.\\n\\n            Rules for planning:\\n            - If a tool\u0027s input depends on the output data of a previous tool, write \\\"{{prev}}\\\" as the placeholder for that data.\\n            - DO NOT try to guess values that depend on previous outputs.\\n            - Use the exact tool names provided above.\\n\\n            Respond ONLY in this pure JSON format:\\n            [\\n            {{ \\\"tool\\\": \\\"\u003ctool name\u003e\\\", \\\"input\\\": \\\"\u003ctool input or \u0027{{prev}}\u0027\u003e\\\" }},\\n            {{ \\\"tool\\\": \\\"\u003ctool name\u003e\\\", \\\"input\\\": \\\"\u003ctool input or \u0027{{prev}}\u0027\u003e\\\" }}\\n            ]\\n            \\\"\\\"\\\"\\n        plan_json=\\\"\\\"\\n        # Get the plan from the LLM and clean up any leading/trailing ```json or ```\\n        for r in self.stream_llm(self.deep_llm, planning_prompt):\\n            # print(r)\\n            yield(r)\\n            plan_json += r\\n\\n        print(f\\\"\\\\n[ Planning Agent ]\\\\nPlan: {plan_json}\\\")\\n\\n        for prefix in (\\\"```json\\\", \\\"```\\\"):\\n            if plan_json.startswith(prefix):\\n                plan_json = plan_json[len(prefix):].strip()\\n        if plan_json.endswith(\\\"```\\\"):\\n            plan_json = plan_json[:-3].strip()\\n\\n        try:\\n            tool_plan = json.loads(plan_json)\\n        except Exception as e:\\n            print(f\\\"Failed to parse tool plan JSON: {e}\\\")\\n            return f\\\"Planning failed: {e} \\\\n\\\\n{plan_json}\\\"\\n\\n        tool_outputs = {}\\n        prev_output = None\\n\\n        for step in tool_plan:\\n            print(f\\\"Executing step: {step}\\\")\\n            yield(f\\\"Executing step: {step}\\\")\\n            tool_name = step.get(\\\"tool\\\")\\n            tool_input = step.get(\\\"input\\\", \\\"\\\")\\n            tool_input = tool_input.replace(\\\"{prev}\\\", str(prev_output if prev_output is not None else \\\"\\\"))\\n            print(f\\\"{tool_name} input: {tool_input}\\\")\\n            yield((f\\\"{tool_name} input: {tool_input}\\\"))\\n            # Find tool\\n            tool = next((t for t in self.tools if t.name == tool_name), None)\\n            if not tool:\\n                tool_outputs[tool_name] = f\\\"Tool not found: {tool_name}\\\"\\n                prev_output = None\\n                continue\\n\\n            try:\\n                if hasattr(tool, \\\"run\\\") and callable(tool.run):\\n                    func = tool.run\\n                elif hasattr(tool, \\\"func\\\") and callable(tool.func):\\n                    func = tool.func\\n                elif callable(tool):\\n                    func = tool\\n                else:\\n                    raise ValueError(f\\\"Tool is not callable\\\")\\n\\n                collected = []\\n                result=\\\"\\\"\\n                try:\\n                    for r in func(tool_input):\\n                        # print(f\\\"Step result: {r}\\\")\\n                        yield r\\n                        collected.append(r)\\n                except TypeError:\\n                    # Not iterable \u2014 call again and yield single result\\n                    result = func(tool_input)\\n                    # print(f\\\"Step result: {result}\\\")\\n                    yield result\\n                else:\\n                    # You can combine collected results here if needed:\\n                    result = \\\"\\\".join(str(c) for c in collected)\\n                    yield result\\n                # store result or return if you want\\n                # tool_outputs[tool_name] = result\\n                prev_output = result\\n                tool_outputs[tool_name] = result\\n                self.save_to_memory(query, tool_outputs[tool_name])\\n\\n            except Exception as e:\\n                tool_outputs[tool_name] = f\\\"Error executing {tool_name}: {e}\\\"\\n                prev_output = None\\n                print(tool_outputs)\\n            \\n            \\n        \\n                \\n\\n        # Merge results into a final answer\\n        merge_prompt = f\\\"\\\"\\\"\\n            The query was: {query}\\n            The following tools were executed with their outputs:\\n            {tool_outputs}\\n\\n            Create a final answer that combines all the results.\\n            \\\"\\\"\\\"\\n        final_answer = self.deep_llm.invoke(merge_prompt)\\n        return final_answer\\n\\n    # --- Run with feedback ---\\n    def run_with_feedback(self, agent_fn, query):\\n        \\\"\\\"\\\"Run an agent function with feedback loop where the reviewer can accept or reject the output.\\\"\\\"\\\"\\n        # First attempt\\n        response = agent_fn(query)\\n        \\n        # Review\\n        review_result = self.review_output(query, response)\\n        if review_result.startswith(\\\"NO\\\"):\\n            feedback = review_result[4:].strip()\\n            print(f\\\"\\\\n[ Reviewer ] Feedback: {feedback}\\\")\\n            # Retry with feedback\\n            retry_prompt = f\\\"{query}\\\\n\\\\nThe reviewer says: {feedback}\\\\nPlease improve your answer.\\\"\\n            response = agent_fn(retry_prompt)\\n        \\n        return response\\n\\n    # --- Coordinator ---\\n    def run(self, query):\\n        self.mem.add_session_memory(self.sess.id, f\\\"{query}\\\", \\\"Query\\\", {\\\"topic\\\": \\\"query\\\"}, promote=True)\\n        # Quick triage with Fast Agent (streamed)\\n        print(\\\"\\\\n[ Triage Agent ]\\\\n\\\")\\n        triage_prompt =\\\"\\\"\\n        fast_response=\\\"\\\"\\n        deep_response=\\\"\\\"\\n        reasoning_response=\\\"\\\"\\n        tool_chain_response=\\\"\\\"\\n        tool_response=\\\"\\\"\\n        # if self.triage_memory is True: triage_prompt += (f\\\"Given the conversation so far: {self.buffer_memory.load_memory_variables({})[\u0027chat_history\u0027]}\\\\n\\\")\\n        # if self.long_term_triage is True: triage_prompt += (f\\\"Given the conversation so far: {self.buffer_memory.load_memory_variables({})[\u0027chat_history\u0027]}\\\\n\\\")\\n        triage_prompt += (\\n                    f\\\"\\\"\\\"\\n                    Classify this Query into one of the following categories:\\n                        - \u0027focus\u0027      \u2192 Change the focus of background thought.\\n                        - \u0027proactive\u0027  \u2192 Trigger proactive thinking.\\n                        - \u0027simple\u0027     \u2192 Simple textual response.\\n                        - \u0027tool\u0027       \u2192 Requires execution of a single tool.\\n                        - \u0027toolchain\u0027  \u2192 Requires a series of tools or step-by-step planning and execution.\\n                        - \u0027reasoning\u0027  \u2192 Requires deep reasoning.\\n                        - \u0027complex\u0027    \u2192 Complex written response with high-quality output.\\n                        - \u0027scheduling\u0027 \u2192 Requires scheduling or time management tasks.\\n\\n                    Current focus: {self.focus_manager.focus}  \\n                    If you detect a change in focus or topic, you may specify new focus terms by appending the output with the.\\n\\n                    Available tools: {\u0027, \u0027.join(t.name for t in self.tools)}\\n\\n                    Query: {query}\\n\\n                    Rules:\\n                    - If \u0027simple\u0027 is the chosen category, disregard these rules and answer the Query using as many words as you like.\\n                    - Respond with a single classification term (e.g., \u0027simple\u0027, \u0027tool\u0027, \u0027complex\u0027) on the first line, then any optional extra info.\\n                    - You may optionally append focus terms.\\n                    - If setting a \u0027focus\u0027, also specify the focus term to set (e.g., \\\"focus project management\\\").\\n                    - Do NOT provide reasoning in your output nor formatting not mentioned in this prompt.\\n                    \\\"\\\"\\\"\\n        )\\n        for r in self.stream_llm(self.fast_llm, triage_prompt):\\n            fast_response += r\\n        self.mem.add_session_memory(self.sess.id, f\\\"{fast_response}\\\", \\\"Triage\\\", {\\\"topic\\\": \\\"fast\\\"}, promote=True)\\n        \\n        if \\\"focus\\\" in fast_response.lower():\\n            # If the query is about focus, use the proactive focus manager\\n            print(\\\"\\\\n[ Proactive Focus Manager ]\\\\n\\\")\\n            self.focus_manager.set_focus(fast_response.lower().split(\\\"focus\\\", 1)[-1].strip())\\n            # proactive_response = self.focus_manager.relate_to_focus(query, fast_response)\\n            # self.save_to_memory(query, proactive_response)\\n            # return {\\\"fast\\\": fast_response}\\n        \\n        # Decide routing\\n        if \\\"complex\\\" in fast_response.lower():\\n            print(\\\"\\\\n[ Deep Agent ]\\\\n\\\")\\n            for r in self.stream_llm_with_memory(self.deep_llm, query, extra_context=fast_response):\\n                deep_response += r\\n            self.save_to_memory(query, deep_response)\\n            self.mem.add_session_memory(self.sess.id, f\\\"{deep_response}\\\", \\\"Response\\\", {\\\"topic\\\": \\\"complex\\\"}, promote=True)\\n            return {\\\"fast\\\": fast_response, \\\"deep\\\": deep_response}\\n        \\n        elif \\\"reasoning\\\" in fast_response.lower():\\n            print(\\\"\\\\n[ Reasoning Agent ]\\\\n\\\")\\n            for r in self.stream_llm_with_memory(self.reasoning_llm, query, extra_context=fast_response):\\n                reasoning_response += r\\n            self.save_to_memory(query, reasoning_response)\\n            self.mem.add_session_memory(self.sess.id, f\\\"{reasoning_response}\\\", \\\"Response\\\", {\\\"topic\\\": \\\"reasoning\\\"}, promote=True)\\n            return {\\\"fast\\\": fast_response, \\\"reasoning\\\": reasoning_response}\\n        \\n        elif \\\"toolchain\\\" in fast_response.lower():\\n            print(\\\"\\\\n[ Tool Chain Agent ]\\\\n\\\")\\n            for r in self.toolchain.execute_tool_chain(query):\\n                tool_chain_response += str(r)\\n            self.save_to_memory(query, tool_chain_response)\\n            self.mem.add_session_memory(self.sess.id, f\\\"{tool_chain_response}\\\", \\\"Response\\\", {\\\"topic\\\": \\\"toolchain\\\"}, promote=True)\\n            return {\\\"fast\\\": fast_response, \\\"toolchain\\\": tool_chain_response}\\n                \\n        elif \\\"tool\\\" in fast_response.lower():\\n            print(\\\"\\\\n[ Tool Agent ]\\\\n\\\")\\n\\n            tool_response = self.light_agent.invoke(query)         \\n            \\n            # Save both the user query and the full tool agent output (including intermediate steps if available)\\n\\n            if hasattr(self.light_agent, \\\"agent_executor\\\") and hasattr(self.light_agent.agent_executor, \\\"intermediate_steps\\\"):\\n                # If intermediate steps are available, save them as well\\n                intermediate_steps = self.light_agent.agent_executor.intermediate_steps\\n                self.save_to_memory(query, {\\\"output\\\": tool_response[\u0027output\u0027], \\\"intermediate_steps\\\": tool_response[\u0027intermediate_steps\u0027]})\\n                return {\\\"fast\\\": fast_response, \\\"tool\\\": tool_response[\u0027output\u0027]}\\n            else:\\n                self.save_to_memory(query, tool_response)\\n                self.mem.add_session_memory(self.sess.id, f\\\"{tool_response}\\\", \\\"Response\\\", {\\\"topic\\\": \\\"tool\\\"}, promote=True)\\n            return {\\\"fast\\\": fast_response, \\\"tool\\\": tool_response}\\n\\n        elif \\\"proactive\\\" in fast_response.lower():\\n            # If the query is about proactive thinking, use the proactive focus manager\\n            print(\\\"\\\\n[ Proactive Focus Manager ]\\\\n\\\")\\n            proactive_thought = self.focus_manager._generate_proactive_thought()\\n            if proactive_thought:\\n                self.focus_manager.add_to_focus_board(\\\"actions\\\", proactive_thought)\\n                self.save_to_memory(query, proactive_thought)\\n                self.mem.add_session_memory(self.sess.id, f\\\"{proactive_thought}\\\", \\\"Thought\\\", {\\\"topic\\\": \\\"proactive\\\"}, promote=True)\\n                return {\\\"fast\\\": fast_response, \\\"proactive\\\": proactive_thought}\\n            else:\\n                return {\\\"fast\\\": fast_response, \\\"proactive\\\": \\\"No proactive thought generated.\\\"}\\n        \\n        elif \\\"scheduling\\\" in fast_response.lower():\\n            print(\\\"\\\\n[ Scheduling Assistant ]\\\\n\\\")\\n            scheduling_response = self.executive_instance.main(query)\\n            self.save_to_memory(query, scheduling_response)\\n            self.mem.add_session_memory(self.sess.id, f\\\"{scheduling_response}\\\", \\\"Response\\\", {\\\"topic\\\": \\\"scheduling\\\"}, promote=True)\\n            return {\\\"fast\\\": fast_response, \\\"scheduling\\\": scheduling_response}\\n        \\n        else:\\n        #     # Simple case, just fast agent again\\n        #     final_fast_response = self.stream_llm_with_memory(self.fast_llm, query, extra_context=fast_response)\\n        #     self.save_to_memory(query, final_fast_response)\\n        #     return {\\\"fast\\\": final_fast_response}\\n            self.save_to_memory(query, fast_response)\\n            return {\\\"fast\\\": fast_response}\\n        \\n        \\n    def async_run(self, query):\\n        self.mem.add_session_memory(self.sess.id, f\\\"{query}\\\", \\\"Query\\\", {\\\"topic\\\": \\\"plan\\\"}, promote=True)\\n        # 1. Stream triage prompt output chunk-by-chunk\\n        triage_prompt = (\\n            f\\\"\\\"\\\"\\n            Classify this Query into one of the following categories:\\n                - \u0027focus\u0027      \u2192 Change the focus of background thought.\\n                - \u0027proactive\u0027  \u2192 Trigger proactive thinking.\\n                - \u0027simple\u0027     \u2192 Simple textual response.\\n                - \u0027tool\u0027       \u2192 Requires execution of a single tool.\\n                - \u0027toolchain\u0027  \u2192 Requires a series of tools or step-by-step planning and execution.\\n                - \u0027reasoning\u0027  \u2192 Requires deep reasoning.\\n                - \u0027complex\u0027    \u2192 Complex written response with high-quality output.\\n\\n            Current focus: {self.focus_manager.focus}  \\n            If you detect a change in focus or topic, you may specify new focus terms by appending the output with the.\\n\\n            Available tools: {\u0027, \u0027.join(t.name for t in self.tools)}\\n\\n            Query: {query}\\n\\n            Rules:\\n            - If \u0027simple\u0027 is the chosen category, disregard these rules and answer the Query using as many words as you like.\\n            - Respond with a single classification term (e.g., \u0027simple\u0027, \u0027tool\u0027, \u0027complex\u0027) on the first line, then any optional extra info.\\n            - You may optionally append focus terms.\\n            - If setting a \u0027focus\u0027, also specify the focus term to set (e.g., \\\"focus project management\\\").\\n            - Do NOT provide reasoning in your output nor formatting not mentioned in this prompt.\\n            \\\"\\\"\\\"\\n        )\\n\\n        # Assume self.stream_llm returns a generator (or wrap to async generator)\\n        full_triage=\\\"\\\"\\n        for triage_chunk in self.stream_llm(self.fast_llm, triage_prompt):\\n            # print(triage_chunk)\\n            full_triage+=triage_chunk\\n            yield triage_chunk\\n        self.mem.add_session_memory(self.sess.id, f\\\"{full_triage}\\\", \\\"Response\\\", {\\\"topic\\\": \\\"triage\\\"}, promote=True)\\n        total_response = full_triage\\n        triage_lower = full_triage.lower()\\n        total_response = \\\"\\\"\\n        fast_response = \\\"\\\"\\n        deep_response = \\\"\\\"\\n        reason_response = \\\"\\\"\\n        tool_response = \\\"\\\"\\n        toolchain_response = \\\"\\\"\\n        \\n        if \\\"focus\\\" in triage_lower:\\n            # If the query is about focus, use the proactive focus manager\\n            print(\\\"\\\\n[ Proactive Focus Manager ]\\\\n\\\")\\n            self.focus_manager.set_focus(full_triage.lower().split(\\\"focus\\\", 1)[-1].strip())\\n            \\n        # 2. Branch logic based on triage\\n        if \\\"simple\\\" in triage_lower:\\n            # stream deep llm output\\n            for fast_chunk in self.stream_llm(self.fast_llm, query): #, extra_context=triage_lower):\\n                fast_response += fast_chunk\\n                yield  fast_chunk\\n            self.mem.add_session_memory(self.sess.id, f\\\"{fast_response}\\\", \\\"Query\\\", {\\\"topic\\\": \\\"query\\\"}) #\\\"model\\\":self.fast_llm})\\n            total_response += fast_response\\n        if \\\"complex\\\" in triage_lower:\\n            # stream deep llm output\\n            for deep_chunk in self.stream_llm(self.deep_llm, query): #, extra_context=triage_lower):\\n                deep_response += deep_chunk\\n                yield  deep_chunk\\n            self.mem.add_session_memory(self.sess.id, f\\\"{deep_response}\\\", \\\"Query\\\", {\\\"topic\\\": \\\"query\\\"}) #\\\"model\\\":self.deep_llm})\\n            total_response += deep_response\\n        elif \\\"reasoning\\\" in triage_lower:\\n            for reason_chunk in self.stream_llm(self.reasoning_llm, query): #, extra_context=triage_lower):\\n                reason_response += reason_chunk\\n                yield reason_chunk\\n            self.mem.add_session_memory(self.sess.id, f\\\"{reason_response}\\\", \\\"Query\\\", {\\\"topic\\\": \\\"query\\\"})\\n            total_response += reason_response\\n        elif \\\"toolchain\\\" in triage_lower:\\n            print(\\\"\\\\n[ Tool Chain Agent ]\\\\n\\\")\\n            # return {\\\"fast\\\": fast_response, \\\"toolchain\\\": tool_chain_response}\\n            for toolchain_chunk in self.toolchain.execute_tool_chain(query):\\n                toolchain_response += str(toolchain_chunk)\\n                yield toolchain_chunk\\n            self.mem.add_session_memory(self.sess.id, f\\\"{toolchain_response}\\\", \\\"Query\\\", {\\\"topic\\\": \\\"query\\\"})\\n            total_response += toolchain_response    \\n            # self.save_to_memory(query, tool_chain_response)\\n        elif \\\"tool\\\" in triage_lower:\\n            for tool_chunk in self.light_agent.invoke(query):\\n                tool_response += str(tool_chunk)\\n                yield tool_chunk\\n            self.mem.add_session_memory(self.sess.id, f\\\"{tool_response}\\\", \\\"Query\\\", {\\\"topic\\\": \\\"query\\\"})\\n            total_response += tool_response      \\n        else:\\n            pass\\n        \\n        self.save_to_memory(query, total_response)\\n        self.mem.add_session_memory(self.sess.id, f\\\"{total_response}\\\", \\\"Response\\\", {\\\"topic\\\": \\\"query\\\"})\\n            \\n\\n\\n    def print_llm_models(self):\\n        \\\"\\\"\\\"Print the variable name and model name for each Ollama LLM.\\\"\\\"\\\"\\n        for attr_name, attr_value in vars(self).items():\\n            if isinstance(attr_value, Ollama):\\n                print(f\\\"{attr_name} -\u003e {attr_value.model}\\\")\\n        \\n    def print_agents(self):\\n        \\\"\\\"\\\"Recursively find and print all LLM models and agents inside Vera.\\\"\\\"\\\"\\n        visited = set()\\n\\n        def inspect_obj(obj, path=\\\"self\\\"):\\n            if id(obj) in visited:\\n                return\\n            visited.add(id(obj))\\n\\n            # Check if this is an agent\\n            if hasattr(obj, \\\"llm\\\") and hasattr(obj.llm, \\\"model\\\"):\\n                agent_type = getattr(obj, \\\"agent\\\", None)\\n                model_name = getattr(obj.llm, \\\"model\\\", \\\"Unknown\\\")\\n                print(f\\\"{path} -\u003e Model: {model_name}, Agent Type: {agent_type}\\\")\\n\\n            # # Check if this is a bare LLM\\n            # elif hasattr(obj, \\\"model\\\") and isinstance(getattr(obj, \\\"model\\\"), str):\\n            #     print(f\\\"{path} -\u003e Model: {obj.model}\\\")\\n\\n            # Recurse into attributes\\n            if hasattr(obj, \\\"__dict__\\\"):\\n                for attr_name, attr_value in vars(obj).items():\\n                    inspect_obj(attr_value, f\\\"{path}.{attr_name}\\\")\\n\\n        inspect_obj(self)\\n\\n    \\n\\nclass ToolChainPlanner:\\n    def __init__(self, agent, tools):\\n        self.agent = agent\\n        self.deep_llm = self.agent.deep_llm\\n        self.tools = self.agent.tools\\n        self.history = self.agent.buffer_memory.load_memory_variables({})[\u0027chat_history\u0027]\\n\\n    def plan_tool_chain(self, query: str, history_context: str = \\\"\\\") -\u003e List[Dict[str, str]]:\\n        \\\"\\\"\\\"Generate a plan from the LLM.\\\"\\\"\\\"\\n        planning_prompt = f\\\"\\\"\\\"\\n            You are a planning assistant.\\n            Available tools: {[(tool.name, tool.description) for tool in self.tools]}.\\n            The query is: {query}\\n\\n            Previous attempts and their outputs:\\\\n{history_context if history_context else \\\"\\\"}\\n\\n            Plan a sequence of tool calls to solve the request.\\n\\n            Rules for planning:\\n            - You can reference ANY previous step output using {{step_1}}, {{step_2}}, etc.\\n            - You can still use {{prev}} to mean the last step\u0027s output.\\n            - DO NOT guess values that depend on previous outputs.\\n            - Use the exact tool names provided above.\\n\\n            Respond ONLY in this pure JSON format, no markdown:\\n            [\\n            {{ \\\"tool\\\": \\\"\u003ctool name\u003e\\\", \\\"input\\\": \\\"\u003ctool input or \u0027{{step_1}}\u0027\u003e\\\" }},\\n            {{ \\\"tool\\\": \\\"\u003ctool name\u003e\\\", \\\"input\\\": \\\"\u003ctool input or \u0027{{prev}}\u0027\u003e\\\" }}\\n            ]\\n        \\\"\\\"\\\"\\n        # plan_json = self.agent.stream_llm_with_memory(self.agent.deep_llm, planning_prompt)\\n        plan_json=\\\"\\\"\\n        # Get the plan from the LLM and clean up any leading/trailing ```json or ```\\n        for r in self.agent.stream_llm(self.deep_llm, planning_prompt):\\n            # print(r)\\n            yield(r)\\n            plan_json += r\\n\\n        # Clean formatting\\n        for prefix in (\\\"```json\\\", \\\"```\\\"):\\n            if plan_json.startswith(prefix):\\n                plan_json = plan_json[len(prefix):].strip()\\n        if plan_json.endswith(\\\"```\\\"):\\n            plan_json = plan_json[:-3].strip()\\n\\n        try:\\n            tool_plan = json.loads(plan_json)\\n        except Exception as e:\\n            raise ValueError(f\\\"Planning failed: {e} \\\\n\\\\n{plan_json}\\\")\\n        print(\\\"DEBUG:\\\", type(tool_plan), tool_plan)\\n        # Save plan for replay\\n        with open(\\\"./last_tool_plan.json\\\", \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n            f.write(plan_json)\\n\\n        if isinstance(tool_plan, dict):\\n            tool_plan = [tool_plan]\\n        elif isinstance(tool_plan, list):\\n            if not all(isinstance(s, dict) for s in tool_plan):\\n                raise ValueError(f\\\"Unexpected tool plan format: {tool_plan}\\\")\\n        else:\\n            raise ValueError(f\\\"Unexpected tool plan type: {type(tool_plan)}\\\")\\n        self.agent.mem.add_session_memory(self.agent.sess.id, f\\\"{json.dumps(tool_plan)}\\\", \\\"Plan\\\", {\\\"topic\\\": \\\"plan\\\"}, promote=True)\\n        yield tool_plan\\n\\n    def execute_tool_chain(self, query: str) -\u003e str:\\n        \\\"\\\"\\\"Execute a tool chain, allowing reference to any step output.\\\"\\\"\\\"\\n        try:\\n            gen = self.plan_tool_chain(query)\\n            for r in gen:\\n                yield r\\n                tool_plan = r\\n        except StopIteration as e:\\n            print(f\\\"[ Toolchain Agent ]\\\\nTool Plan: {json.dumps(e.value, indent=2)}\\\")\\n            tool_plan = e.value\\n        except Exception as e:\\n            print(f\\\"[ Toolchain Agent ] Error planning tool chain: {e}\\\")\\n        print(f\\\"[ Toolchain Agent ]\\\\nTool Plan: {json.dumps(tool_plan, indent=2)}\\\")\\n        # tool_plan = self.plan_tool_chain(query)\\n        tool_outputs = {}\\n        step_num = 0\\n        errors_detected = False\\n\\n        for step in tool_plan:\\n            step_num += 1\\n            tool_name = step.get(\\\"tool\\\")\\n            tool_input = str(step.get(\\\"input\\\", \\\"\\\"))\\n            # yield(f\\\"\\\\nExecuting step: {step}\\\\n{tool_name} input: {tool_input}\\\"))\\n            # Resolve placeholders like {prev} or {step_2}\\n            if \\\"{prev}\\\" in tool_input:\\n                tool_input = tool_input.replace(\\\"{prev}\\\", str(tool_outputs.get(f\\\"step_{step_num-1}\\\", \\\"\\\")))\\n            for i in range(1, step_num):\\n                tool_input = tool_input.replace(f\\\"{{step_{i}}}\\\", str(tool_outputs.get(f\\\"step_{i}\\\", \\\"\\\")))\\n\\n            # Inject memory for LLM tools\\n            if \\\"llm\\\" in tool_name:\\n                tool_input = f\\\"Context: {self.agent.buffer_memory.load_memory_variables({})[\u0027chat_history\u0027]}\\\\n\\\" + tool_input\\n\\n            print(f\\\"[ Toolchain Agent ] Step {step_num} - Executing Tool: {tool_name}, Input: {tool_input}\\\")\\n            yield(f\\\"[ Toolchain Agent ] Step {step_num} - Executing Tool: {tool_name}, Input: {tool_input}\\\")\\n            tool = next((t for t in self.tools if t.name == tool_name), None)\\n\\n            if not tool:\\n                result = f\\\"Tool not found: {tool_name}\\\"\\n                errors_detected = True\\n            else:\\n                try:\\n                    if hasattr(tool, \\\"run\\\") and callable(tool.run):\\n                        func = tool.run\\n                    elif hasattr(tool, \\\"func\\\") and callable(tool.func):\\n                        func = tool.func\\n                    elif callable(tool):\\n                        func = tool\\n                    else:\\n                        raise ValueError(f\\\"Tool is not callable\\\")\\n\\n                    collected = []\\n                    result=\\\"\\\"\\n                    try:\\n                        for r in func(tool_input):\\n                            # print(f\\\"Step result: {r}\\\")\\n                            yield r\\n                            collected.append(r)\\n                    except TypeError:\\n                        # Not iterable \u2014 call again and yield single result\\n                        result = func(tool_input)\\n                        # print(f\\\"Step result: {result}\\\")\\n                        yield result\\n                    else:\\n                        # Combine collected results here if needed:\\n                        result = \\\"\\\".join(str(c) for c in collected)\\n                        self.agent.mem.add_session_memory(self.agent.sess.id, f\\\"Step {step_num} - {tool_name}\\\", result, \\\"Step\\\", {\\\"topic\\\": \\\"toolchain\\\"})\\n                        yield result\\n                    # store result or return if you want\\n                    # tool_outputs[tool_name] = result\\n                    prev_output = result\\n                    tool_outputs[tool_name] = result\\n                    # self.save_to_memory(query, tool_outputs[tool_name])\\n                except Exception as e:\\n                    tool_outputs[tool_name] = f\\\"Error executing {tool_name}: {e}\\\"\\n                    prev_output = None\\n                    print(tool_outputs)\\n            \\n                # try:\\n                #     if hasattr(tool, \\\"run\\\") and callable(tool.run):\\n                #         result = tool.run(tool_input)\\n                #     elif hasattr(tool, \\\"func\\\") and callable(tool.func):\\n                #         result = tool.func(tool_input)\\n                #     elif callable(tool):\\n                #         result = tool(tool_input)\\n                #     else:\\n                #         raise ValueError(f\\\"Tool {tool_name} is not callable.\\\")\\n                # except Exception as e:\\n                #     result = f\\\"Error executing {tool_name}: {str(e)}\\\"\\n                #     errors_detected = True\\n\\n                print(f\\\"Step {step_num} result: {result}\\\")\\n                yield(f\\\"Step {step_num} result: {result}\\\")\\n                tool_outputs[f\\\"step_{step_num}\\\"] = result\\n                self.agent.save_to_memory(f\\\"Step {step_num} - {tool_name}\\\", result)\\n                self.agent.mem.add_session_memory(self.agent.sess.id, f\\\"Step {step_num} - {tool_name}: {result}\\\", \\\"Step\\\", {\\\"topic\\\": \\\"toolchain\\\"})\\n\\n        # If error detected \u2192 re-plan recovery step\\n        if errors_detected:\\n            print(\\\"[ Toolchain Agent ] Errors detected, replanning recovery steps...\\\")\\n            recovery_plan = self.plan_tool_chain(\\n                f\\\"Recover from the errors and complete the query: {query}\\\",\\n                history_context=json.dumps(tool_outputs, indent=2)\\n            )\\n            tool_plan.extend(recovery_plan)\\n            return self.execute_tool_chain(query)  # re-run with recovery\\n\\n        # Final goal check\\n        review_prompt = f\\\"\\\"\\\"\\n            The query was: {query}\\n            Execution results: {json.dumps(tool_outputs, indent=2)}\\n\\n            Does the final result meet the goal? \\n            Answer only \u0027yes\u0027 or \u0027no\u0027 and explain briefly.\\n        \\\"\\\"\\\"\\n        review = self.deep_llm.invoke(review_prompt)\\n        if \\\"no\\\" in review.lower():\\n            print(\\\"[ Toolchain Agent ] Goal not achieved, replanning...\\\")\\n            retry_plan = self.plan_tool_chain(\\n                f\\\"Retry the task ensuring the goal is met. Original query: {query}\\\",\\n                history_context=json.dumps(tool_outputs, indent=2)\\n            )\\n            return self.execute_tool_chain(query)\\n\\n        return tool_outputs.get(f\\\"step_{step_num}\\\", \\\"\\\")\\n\\n    def report_history(self) -\u003e str:\\n        \\\"\\\"\\\"Generate a report of all tool chains run so far.\\\"\\\"\\\"\\n        report_prompt = f\\\"\\\"\\\"\\n            You are a summarization assistant.\\n            Here is the short-term memory of all executed tool chains:\\n\\n            {json.dumps(self.history, indent=2)}\\n\\n            Please produce a clear and concise report that summarizes:\\n            - Each query and the plan used\\n            - Key results\\n            - Any patterns or common findings\\n        \\\"\\\"\\\"\\n        return self.deep_llm.invoke(report_prompt)\\n\\nclass ProactiveFocusManager:\\n    \\\"\\\"\\\"\\n    TODO\\n    Add calendar visibility\\n    \\\"\\\"\\\"\\n    def __init__(\\n        self,\\n        agent,\\n        proactive_interval: int = 60*10,  # seconds between proactive thoughts\\n        cpu_threshold: float = 30.0,    # max CPU percent usage allowed for proactive thinking\\n    ):\\n        self.agent = agent\\n        self.focus: Optional[str] = None\\n        self.focus_board = {\\n            \\\"progress\\\": [],\\n            \\\"next_steps\\\": [],\\n            \\\"issues\\\": [],\\n            \\\"ideas\\\": [],\\n            \\\"actions\\\": []\\n        }\\n        self.proactive_interval = proactive_interval\\n        self.cpu_threshold = cpu_threshold\\n        self.running = False\\n        self.thread = None\\n        self.latest_conversation = \\\"\\\"  # Current conversation/context to steer thoughts\\n        self.proactive_callback: Optional[Callable[[str], None]] = None  # Optional callback on new proactive thought\\n        self.max_ollama_processes = 24\\n        self.pause_event = threading.Event()  # Pauses proactive thinking\\n\\n\\n    def set_focus(self, focus: str):\\n        self.focus = focus\\n        print(f\\\"[FocusManager] Focus set to: {focus}\\\")\\n        self.agent.mem.add_session_memory(self.agent.sess.id, f\\\"[FocusManager] Focus set to: {focus}\\\", \\\"Thought\\\", {\\\"topic\\\": \\\"focus\\\"})\\n        # self.start()\\n\\n    def clear_focus(self):\\n        self.focus = None\\n        self.stop()\\n        print(\\\"[FocusManager] Focus cleared\\\")\\n\\n    def add_to_focus_board(self, category: str, note: str):\\n        if category in self.focus_board:\\n            self.focus_board[category].append(note)\\n        else:\\n            self.focus_board[category] = [note]\\n\\n    def update_latest_conversation(self, conversation: str):\\n        self.latest_conversation = conversation\\n\\n    def start(self):\\n        if not self.running and self.focus:\\n            self.running = True\\n            self.thread = threading.Thread(target=self._run_proactive_loop, daemon=True)\\n            self.thread.start()\\n\\n    def stop(self):\\n        self.running = False\\n        if self.thread:\\n            self.thread.join(timeout=1)\\n            self.thread = None\\n\\n    def _count_ollama_processes(self):\\n        count = 0\\n        for proc in psutil.process_iter(attrs=[\\\"name\\\"]):\\n            try:\\n                if \\\"ollama\\\" in proc.info[\\\"name\\\"].lower():\\n                    count += 1\\n            except (psutil.NoSuchProcess, psutil.AccessDenied):\\n                continue\\n        return count\\n\\n    def _run_proactive_loop(self):\\n        print(\\\"[FocusManager] Proactive loop started\\\")\\n        while self.running:\\n            ollama_count = self._count_ollama_processes()\\n            if ollama_count \u003e= self.max_ollama_processes:\\n                print(f\\\"[FocusManager] High Ollama demand ({ollama_count} processes) \u2014 pausing proactive thinking...\\\")\\n                self.pause_event.clear()\\n                while self.running and self._count_ollama_processes() \u003e self.max_ollama_processes:\\n                    time.sleep(2)\\n                print(\\\"[FocusManager] Ollama demand dropped \u2014 resuming proactive thinking...\\\")\\n                self.pause_event.set()\\n\\n            self.pause_event.wait()\\n\\n            proactive_thought = self._generate_proactive_thought()\\n            \\n            if proactive_thought:\\n                if self.proactive_callback:\\n                    self.proactive_callback(proactive_thought)\\n                self.add_to_focus_board(\\\"actions\\\", proactive_thought)\\n                # print(f\\\"[FocusManager] Proactive action/thought: {proactive_thought}\\\")\\n                evaluation_prompt = (\\n                    f\\\"Evaluate this proactive thought: {proactive_thought}\\\\n\\\"\\n                    f\\\"Is it actionable given the tools available and relevant to the current focus?\\\"\\n                    f\\\"The tools available are: {[tool.name for tool in self.agent.tools]}\\\\n\\\"\\n                    f\\\"The focus is: {self.focus}\\\\n\\\"\\n                    f\\\"If so, respond with \u0027YES\u0027. If not, provide a brief reason.\\\"\\n                )\\n                evaluation = self.agent.fast_llm.invoke(evaluation_prompt)\\n                if evaluation.strip().lower() == \\\"yes\\\":\\n                    self.execute_goal_with_vera(proactive_thought)\\n\\n\\n            time.sleep(self.proactive_interval)\\n\\n    def _generate_proactive_thought(self) -\u003e Optional[str]:\\n        print(\\\"[FocusManager] Generating proactive thought...\\\")\\n        if not self.focus:\\n            return None\\n        # Compose prompt leveraging focus + latest conversation for actionable advice\\n        prompt = (\\n            f\\\"You are assisting with the project: {self.focus}.\\\\n\\\"\\n            f\\\"Based on this recent conversation/context:\\\\n{self.latest_conversation}\\\\n\\\"\\n            f\\\"Considering the focus board:\\\\n{self.focus_board}\\\\n\\\"\\n            f\\\"Suggest the most valuable immediate action or next step to advance the project. \\\"\\n            f\\\"Focus on concrete, practical actions or investigations.\\\"\\n        )\\n        try:\\n            # Use the deep agent (or intermediate) for better reasoning\\n            # Use streaming disabled here; you want the full response before acting\\n            response = self.agent.deep_llm.predict(prompt)\\n            return response.strip()\\n        except Exception as e:\\n            print(f\\\"[FocusManager] Error generating proactive thought: {e}\\\")\\n            return None\\n            \\n    def execute_goal_with_vera(self, goal: str):\\n        \\\"\\\"\\\"Instruct Vera to achieve the goal using tools if needed and log results.\\\"\\\"\\\"\\n        try:\\n            print(f\\\"[FocusManager] Sending goal to Vera: {goal}\\\")\\n\\n            for r in self.toolchain.execute_tool_chain(f\\\"Goal: {goal}\\\\n\\\\nFocus: {self.focus}\\\\n\\\\nStatus: {self.focus_board}\\\"):\\n                tool_chain_response += str(r)\\n\\n            # Superseded\\n            # result = self.agent.execute_tool_chain (\\n            #     f\\\"Goal: {goal}\\\\n\\\\nFocus: {self.focus}\\\\n\\\\nStatus: {self.focus_board}\\\"\\n            # )\\n\\n            # Store results in focus board\\n            if result:\\n                self.add_to_focus_board(\\\"progress\\\", f\\\"Executed goal: {goal}\\\")\\n                self.add_to_focus_board(\\\"progress\\\", f\\\"Result: {result}\\\")\\n\\n                # If result contains substeps or breakdown, categorize accordingly\\n                if isinstance(result, dict):\\n                    if \\\"next_steps\\\" in result:\\n                        for step in result[\\\"next_steps\\\"]:\\n                            self.add_to_focus_board(\\\"next_steps\\\", step)\\n                    if \\\"issues\\\" in result:\\n                        for issue in result[\\\"issues\\\"]:\\n                            self.add_to_focus_board(\\\"issues\\\", issue)\\n                    if \\\"ideas\\\" in result:\\n                        for idea in result[\\\"ideas\\\"]:\\n                            self.add_to_focus_board(\\\"ideas\\\", idea)\\n\\n                print(f\\\"[FocusManager] Logged results to focus board.\\\")\\n\\n        except Exception as e:\\n            print(f\\\"[FocusManager] Failed to execute goal with Vera: {e}\\\")\\n            self.add_to_focus_board(\\\"issues\\\", f\\\"Execution failed for \u0027{goal}\u0027: {e}\\\")\\n\\n    def relate_to_focus(self, user_input: str, response: str) -\u003e str:\\n        # Optionally, can connect any response back to focus by appending a reminder or summary\\n        if not self.focus:\\n            return response\\n        return f\\\"{response}\\\\n\\\\n[Reminder: Current project focus is \u0027{self.focus}\u0027]\\\"\\n\\n\\ndef get_active_ollama_threads():\\n    \\\"\\\"\\\"Return active Ollama threads with non-zero CPU usage.\\\"\\\"\\\"\\n    active_threads = []\\n    total_cpu = 0.0\\n\\n    for proc in psutil.process_iter(attrs=[\\\"pid\\\", \\\"name\\\", \\\"cmdline\\\"]):\\n        try:\\n            if \\\"ollama\\\" in (proc.info[\\\"name\\\"] or \\\"\\\").lower() or any(\\\"ollama\\\" in (part or \\\"\\\").lower() for part in proc.info[\\\"cmdline\\\"] or []):\\n                for thread in proc.threads():\\n                    thread_cpu = proc.cpu_percent(interval=0.1) / proc.num_threads() if proc.num_threads() else 0\\n                    if thread_cpu \u003e 0:  # Thread is actively using CPU\\n                        active_threads.append({\\n                            \\\"pid\\\": proc.pid,\\n                            \\\"tid\\\": thread.id,\\n                            \\\"cpu\\\": thread_cpu\\n                        })\\n                        total_cpu += thread_cpu\\n        except (psutil.NoSuchProcess, psutil.AccessDenied):\\n            continue\\n\\n    print(\\\"Active Ollama Threads\\\")\\n    print(\\\"---------------------\\\")\\n    for t in active_threads:\\n        print(f\\\"PID: {t[\u0027pid\u0027]}, TID: {t[\u0027tid\u0027]}, CPU: {t[\u0027cpu\u0027]:.2f}%\\\")\\n    print(\\\"---------------------\\\")\\n    print(f\\\"Total active threads: {len(active_threads)} | Total active CPU: {total_cpu:.2f}%\\\")\\n\\n    return active_threads\\n\\ndef get_ollama_cpu_load_and_count():\\n    \\\"\\\"\\\"Calculate total CPU load and count of threads for all Ollama models in Vera.\\\"\\\"\\\"\\n    total_cpu = 0.0\\n    total_threads = 0\\n    model_processes = {}\\n\\n    for proc in psutil.process_iter(attrs=[\\\"pid\\\", \\\"name\\\", \\\"cmdline\\\", \\\"cpu_percent\\\", \\\"num_threads\\\"]):\\n        try:\\n            name = proc.info[\\\"name\\\"] or \\\"\\\"\\n            cmdline = proc.info[\\\"cmdline\\\"] or []\\n            cpu = proc.info[\\\"cpu_percent\\\"]\\n            threads = proc.info[\\\"num_threads\\\"]\\n\\n            # Look for \u0027ollama\u0027 process\\n            if \\\"ollama\\\" in name.lower() or any(\\\"ollama\\\" in part.lower() for part in cmdline):\\n                total_cpu += cpu\\n                total_threads += threads\\n\\n                # Try to guess the model name from cmdline\\n                model_name = None\\n                for part in cmdline:\\n                    if re.match(r\\\"^[a-zA-Z0-9_\\\\-:]+$\\\", part) and \\\":\\\" in part:\\n                        model_name = part\\n                        break\\n\\n                if not model_name:\\n                    model_name = \\\"unknown\\\"\\n\\n                model_processes.setdefault(model_name, {\\\"cpu\\\": 0.0, \\\"threads\\\": 0})\\n                model_processes[model_name][\\\"cpu\\\"] += cpu\\n                model_processes[model_name][\\\"threads\\\"] += threads\\n\\n        except (psutil.NoSuchProcess, psutil.AccessDenied):\\n            continue\\n\\n    print(\\\"Ollama CPU Load Report\\\")\\n    print(\\\"----------------------\\\")\\n    for model, stats in model_processes.items():\\n        print(f\\\"{model} -\u003e CPU: {stats[\u0027cpu\u0027]:.2f}%, Threads: {stats[\u0027threads\u0027]}\\\")\\n    print(\\\"----------------------\\\")\\n    print(f\\\"TOTAL -\u003e CPU: {total_cpu:.2f}%, Threads: {total_threads}\\\")\\n\\n\\n# --- Example usage ---\\nif __name__ == \\\"__main__\\\":\\n    \\n    vera = Vera()\\n    \\n    os.system(\\\"clear\\\")\\n    vera.print_llm_models()\\n    vera.print_agents()\\n    # get_ollama_cpu_load_and_count()\\n    # get_active_ollama_threads()\\n\\n    while True:\\n        user_query = input(\\\"\\\\nEnter your query (or \u0027exit\u0027 to quit):\\\\n\\\\n \\\")\\n        if user_query.lower() in [\\\"exit\\\", \\\"quit\\\"]:\\n            break\\n        print(\\\"\\\\nRunning agent...\\\")\\n        \\n        if user_query.lower() == \\\"/clear\\\":\\n            vera.vectorstore.delete_collection(\\\"vera_agent_memory\\\")\\n            vera.buffer_memory.clear()\\n            print(\\\"Memory cleared.\\\")\\n        if user_query.lower() == \\\"/test\\\":\\n            print(\\\"Testing tool execution...\\\")\\n            print(vera.execute_tool_chain(\\\"List all projects\\\"))\\n            print(vera.execute_tool_chain(\\\"Add a new event to Google Calendar for tomorrow at 10 AM\\\"))\\n        if user_query.lower() == \\\"/run\\\":\\n            print(\\\"Running a test query...\\\")\\n            print(vera.run(\\\"What is the weather like today?\\\"))\\n        # if user_query.lower() == \\\"/replay\\\":\\n        # if user_query.lower() == \\\"/search\\\":\\n        # if user_query.lower() == \\\"/history\\\":\\n        # if user_query.lower() == \\\"/model\\\": \\n        # if user_query.lower() == \\\"/focus\\\":\\n        # if user_query.lower() == \\\"/proactive\\\":\\n            # proactive_thought = self.focus_manager._generate_proactive_thought()\\n            # if proactive_thought:\\n            #     self.focus_manager.add_to_focus_board(\\\"actions\\\", proactive_thought)\\n            #     self.save_to_memory(query, proactive_thought)\\n\\n            # else :\\n            #     result = vera.run(user_query)\\n\\n        # print(\\\"\\\\n[ Results ]\\\")\\n        # print(\\\"Fast Response:\\\", result.get(\\\"fast\\\", \\\"No fast response\\\"))\\n        # print(\\\"Deep Response:\\\", result.get(\\\"deep\\\", \\\"No deep response\\\"))\\n        # print(\\\"Tool Response:\\\", result.get(\\\"tool\\\", \\\"No tool response\\\"))\\n        # print(\\\"\\\\n---\\\\n\\\")\\n        result = vera.run(user_query)\\n        # get_ollama_cpu_load_and_count()\\n        print(result)\\n\\n# \u30b8\u30e7\u30bb\u30d5\",\n    \"type\": \"Step\"\n  },\n  \"labels\": [\n    \"Step\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFB3BA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:37", "label": "Investigate API rate limits ...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"risk\",\n    \"id\": \"mem_1755548220204\",\n    \"text\": \"Investigate API rate limits for upstream service.\",\n    \"type\": \"thought\"\n  },\n  \"labels\": [\n    \"Thought\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFB3BA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:38", "label": "Decision: use exponential ba...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"decision\",\n    \"id\": \"mem_1755548220965\",\n    \"text\": \"Decision: use exponential backoff with jitter.\",\n    \"type\": \"thought\"\n  },\n  \"labels\": [\n    \"Thought\"\n  ]\n}"}, {"borderWidth": 1, "color": "#E6BAFF", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:39", "label": "Step 2 - Query Fast LLM: ", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"toolchain\",\n    \"id\": \"mem_1755734625969\",\n    \"text\": \"Step 2 - Query Fast LLM: \",\n    \"type\": \"Step\"\n  },\n  \"labels\": [\n    \"Step\"\n  ]\n}"}, {"borderWidth": 1, "color": "#FFB3BA", "font": {"color": "white"}, "id": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:40", "label": "Investigate API rate limits ...", "mass": 1.1, "shape": "dot", "size": 18, "title": "{\n  \"properties\": {\n    \"topic\": \"risk\",\n    \"id\": \"mem_1755548414275\",\n    \"text\": \"Investigate API rate limits for upstream service.\",\n    \"type\": \"thought\"\n  },\n  \"labels\": [\n    \"Thought\"\n  ]\n}"}]);
                  edges = new vis.DataSet([{"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:13", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:0"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:268", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:1"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:6003", "label": "", "length": 280, "title": "ORPHAN_LINK", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:2"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:6003", "label": "", "length": 280, "title": "ORPHAN_LINK", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:3"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:0", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:4"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:1", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:5"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:6003", "label": "", "length": 280, "title": "ORPHAN_LINK", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:6"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:6003", "label": "", "length": 280, "title": "ORPHAN_LINK", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:7"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:4", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:8"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:5", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:9"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:6003", "label": "", "length": 280, "title": "ORPHAN_LINK", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:10"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:239", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:11"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:12", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:14"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:266", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:15"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:14", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:16"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:6003", "label": "", "length": 280, "title": "ORPHAN_LINK", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:17"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:16", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:18"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:18", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:19"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:19", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:20"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:20", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:21"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:6003", "label": "", "length": 280, "title": "ORPHAN_LINK", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:22"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:1336", "label": "", "length": 200, "title": "HAS_THOUGHT", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:23"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:1335", "label": "", "length": 200, "title": "HAS_THOUGHT", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:23"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:1326", "label": "", "length": 200, "title": "HAS_THOUGHT", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:23"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:1325", "label": "", "length": 200, "title": "HAS_THOUGHT", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:23"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:79", "label": "", "length": 200, "title": "HAS_THOUGHT", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:23"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:78", "label": "", "length": 200, "title": "HAS_THOUGHT", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:23"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:47", "label": "", "length": 200, "title": "HAS_THOUGHT", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:23"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:46", "label": "", "length": 200, "title": "HAS_THOUGHT", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:23"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:25", "label": "", "length": 200, "title": "HAS_DOCUMENT", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:23"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:24", "label": "", "length": 200, "title": "DEPENDS_ON", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:23"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:21", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:26"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:6003", "label": "", "length": 280, "title": "ORPHAN_LINK", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:27"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:6003", "label": "", "length": 280, "title": "ORPHAN_LINK", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:28"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:26", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:29"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:6003", "label": "", "length": 280, "title": "ORPHAN_LINK", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:30"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:29", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:31"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:6003", "label": "", "length": 280, "title": "ORPHAN_LINK", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:32"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:11", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:33"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:6003", "label": "", "length": 280, "title": "ORPHAN_LINK", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:34"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:6003", "label": "", "length": 280, "title": "ORPHAN_LINK", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:35"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:31", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:36"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:6003", "label": "", "length": 280, "title": "ORPHAN_LINK", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:37"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:6003", "label": "", "length": 280, "title": "ORPHAN_LINK", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:38"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:36", "label": "", "length": 200, "title": "FOLLOWS", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:39"}, {"arrows": "to", "from": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:6003", "label": "", "length": 280, "title": "ORPHAN_LINK", "to": "4:0658720f-ad9e-48e1-a121-81d15220aeb0:40"}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {"interaction": {"hover": true, "navigationButtons": true, "zoomView": true}, "edges": {"smooth": {"enabled": true, "type": "dynamic"}, "arrows": {"to": {"enabled": true}}}, "layout": {"improvedLayout": false}, "physics": {"enabled": true, "stabilization": {"enabled": true, "iterations": 300}, "barnesHut": {"gravitationalConstant": -9000, "centralGravity": 0.06, "springLength": 300, "springConstant": 0.01, "damping": 0.62}}};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  
                  // make a custom popup
                      var popup = document.createElement("div");
                      popup.className = 'popup';
                      popupTimeout = null;
                      popup.addEventListener('mouseover', function () {
                          console.log(popup)
                          if (popupTimeout !== null) {
                              clearTimeout(popupTimeout);
                              popupTimeout = null;
                          }
                      });
                      popup.addEventListener('mouseout', function () {
                          if (popupTimeout === null) {
                              hidePopup();
                          }
                      });
                      container.appendChild(popup);


                      // use the popup event to show
                      network.on("showPopup", function (params) {
                          showPopup(params);
                      });

                      // use the hide event to hide it
                      network.on("hidePopup", function (params) {
                          hidePopup();
                      });

                      // hiding the popup through css
                      function hidePopup() {
                          popupTimeout = setTimeout(function () { popup.style.display = 'none'; }, 500);
                      }

                      // showing the popup
                      function showPopup(nodeId) {
                          // get the data from the vis.DataSet
                          var nodeData = nodes.get([nodeId]);
                          popup.innerHTML = nodeData[0].title;

                          // get the position of the node
                          var posCanvas = network.getPositions([nodeId])[nodeId];

                          // get the bounding box of the node
                          var boundingBox = network.getBoundingBox(nodeId);

                          //position tooltip:
                          posCanvas.x = posCanvas.x + 0.5 * (boundingBox.right - boundingBox.left);

                          // convert coordinates to the DOM space
                          var posDOM = network.canvasToDOM(posCanvas);

                          // Give it an offset
                          posDOM.x += 10;
                          posDOM.y -= 20;

                          // show and place the tooltip.
                          popup.style.display = 'block';
                          popup.style.top = posDOM.y + 'px';
                          popup.style.left = posDOM.x + 'px';
                      }
                  


                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>