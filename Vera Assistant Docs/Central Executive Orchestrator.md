
---

# ProactiveFocusManager

**ProactiveFocusManager** is an autonomous background cognition engine that runs on top of `PriorityWorkerPool`. It continuously monitors project context, generates actionable tasks via LLMs, validates them, and executes them through your toolchain while logging progress to a focus board.

It is designed to integrate seamlessly with local, remote, and Proxmox-based worker nodes, providing a distributed, scalable, and high-throughput execution environment.

---

## Features

- **Context-aware task generation:** Pulls context from multiple providers (`ConversationProvider`, `FocusBoardProvider`, or custom providers)
    
- **LLM-driven reasoning:** Uses a deep LLM to generate actionable next steps
    
- **Action validation:** Uses a fast LLM to check if proposed actions are executable
    
- **Distributed execution:** Integrates with local pools, remote HTTP workers, and Proxmox-hosted nodes
    
- **Focus tracking:** Maintains a focus board with progress, next steps, ideas, actions, and issues
    
- **Non-blocking scheduling:** Periodic autonomous ticks with configurable intervals
    

---

## Installation

```bash
# Ensure your environment has the worker tools
pip install requests psutil
# Include your worker modules
# - worker_pool.py
# - proxmox_worker_manager.py
# - proxmox_autoscale_cluster.py
```

---

## Usage

### Basic setup with a local worker pool

```python
from worker_pool import PriorityWorkerPool
from proactive_focus import ProactiveFocusManager

local_pool = PriorityWorkerPool(worker_count=4)
local_pool.set_concurrency_limit("llm", 2)
local_pool.set_concurrency_limit("exec", 1)
local_pool.start()

pfm = ProactiveFocusManager(agent=my_agent, pool=local_pool, proactive_interval=300)
pfm.set_focus("Improve developer onboarding docs")
pfm.start()
```

- `my_agent` should provide:
    
    - `deep_llm.predict(prompt)` → generate candidate actions
        
    - `fast_llm.invoke(prompt)` → evaluate action feasibility
        
    - `toolchain.execute_tool_chain(payload)` → execute actionable plans
        
    - Optional memory: `agent.mem.add_session_memory(...)`
        

---

### Integration with `ClusterWorkerPool` (remote nodes)

```python
from cluster import ClusterWorkerPool, RemoteNode

cluster = ClusterWorkerPool(local_pool)
cluster.add_node(RemoteNode(name="light", base_url="http://192.168.1.20:8081", labels=("llm_light", "llm"), auth_token="secret"))
cluster.add_node(RemoteNode(name="heavy", base_url="http://heavy-host:8082", labels=("llm_heavy", "exec"), auth_token="secret"))

pfm = ProactiveFocusManager(agent=my_agent, pool=cluster.local)
pfm.start()
```

- Tasks generated by `ProactiveFocusManager` automatically go through the local pool.
    
- If desired, you can wrap cluster submission for offloading to remote nodes.
    

---

### Integration with `proxmox_worker_manager` & `proxmox_autoscale_cluster`

```python
from proxmox_worker_manager import ProxmoxWorkerManager
from proxmox_autoscale_cluster import ProxmoxAutoClusterPool

proxmox_mgr = ProxmoxWorkerManager(api_host="pve.local", user="root@pam", password="secret")
auto_cluster = ProxmoxAutoClusterPool(local_pool=local_pool, proxmox_manager=proxmox_mgr, max_workers=5)
auto_cluster.start_autoscaler()

pfm = ProactiveFocusManager(agent=my_agent, pool=auto_cluster.local_pool)
pfm.start()
```

- Automatically scales Proxmox VMs/containers up or down based on load.
    
- Works alongside non-Proxmox hosts to distribute workload intelligently.
    
- Ensures low-latency LLM tasks run locally or on lightweight nodes while heavy tasks run on scalable Proxmox workers.
    

---

## Context Providers

- **ConversationProvider:** Tracks the latest conversation and provides it as context
    
- **FocusBoardProvider:** Provides current focus board status
    
- **Custom providers:** Implement `ContextProvider.collect()` to add arbitrary context
    

---

## Focus Board

- Categories: `progress`, `next_steps`, `issues`, `ideas`, `actions`
    
- Updated automatically by task generation and execution
    
- Accessible at `pfm.focus_board`
    

---

## Lifecycle

- `pfm.start()` → begins autonomous tick cycle
    
- `pfm.stop()` → stops background processing
    
- `_tick()` → collects context, generates thoughts, validates, executes, and schedules next tick
    
- `_generate_and_route(ctx)` → submits actionable goals to the worker pool
    
- `_execute_goal(goal)` → executes tasks via agent’s toolchain
    

---

## Use Cases

|Use Case|Integration|
|---|---|
|**Local CPU-bound tasks**|Runs directly on `PriorityWorkerPool`, respects CPU and concurrency limits|
|**Remote nodes**|Uses `ClusterWorkerPool` + `RemoteNode` to offload tasks to HTTP-exposed worker pools|
|**Proxmox auto-scaling**|`ProxmoxAutoClusterPool` dynamically spins up/down VMs or containers and integrates with local pool|
|**Mixed clusters**|Supports hybrid environments: local + remote + Proxmox nodes|
|**LangChain agents**|Wrap `_submit_task` or cluster submission as a LangChain `Tool`|

---

## Notes

- Labels are critical: `"llm"`, `"exec"`, `"llm_light"`, `"llm_heavy"` allow workload routing
    
- `_generate_proactive_thought` and `_is_actionable` rely on your agent’s LLM tools
    
- Focus board updates asynchronously; read/write safely if accessed externally
    
- Works transparently with local, remote, or Proxmox-backed worker pools
    

---

This README now fully describes **ProactiveFocusManager** and how it fits with:

- `PriorityWorkerPool` (local)
    
- `ClusterWorkerPool` (remote)
    
- `proxmox_worker_manager` + `proxmox_autoscale_cluster` (dynamic Proxmox scaling)
    

---

# Proxmox Worker Modules & Autoscaling Cluster

This README covers:

- `proxmox_worker_manager.py` – Manages Proxmox VMs/containers as workers.
    
- `proxmox_autoscale_cluster.py` – Integrates Proxmox workers with `ClusterWorkerPool` and local pools, automatically scaling them based on load.
    
- How both modules integrate with `worker_pool` and LangChain workflows.
    

---

## Overview

These modules allow you to run **distributed worker tasks** across:

1. **Local PriorityWorkerPool** – CPU/resource aware, handles priorities, retries, backoff, and rate limiting.
    
2. **Remote nodes** – HTTP/gRPC workers that can be on any machine.
    
3. **Proxmox workers** – Dynamically provisioned VMs or containers in your Proxmox cluster. Can auto-scale based on workload.
    

This architecture ensures:

- High throughput for CPU-intensive tasks (e.g., LLM inference, tool execution)
    
- Load distribution between lightweight & heavy workloads
    
- Full integration with LangChain as a Python tool
    

---

## Module: `proxmox_worker_manager.py`

### Purpose

Manages Proxmox nodes as workers:

- Provision new VM/container as worker
    
- Teardown when idle or scaled down
    
- Track remote worker nodes via SSH or Proxmox API
    

### Key Classes

- **`ProxmoxWorkerManager`** – Main interface to create and manage worker nodes.
    
- **`ProxmoxRemoteNode`** – Represents a single worker VM/container, callable from `ClusterWorkerPool`.
    

### Example

```python
from proxmox_worker_manager import ProxmoxWorkerManager

manager = ProxmoxWorkerManager(api_host="pve.local", user="root@pam", password="secret")

# Provision a new worker VM
worker_name = manager.provision_worker(vmid=1001, node_name="pve", labels=("llm",))

# List active workers
workers = manager.list_workers()

# Tear down a worker when done
manager.teardown_worker(worker_name)
```

---

## Module: `proxmox_autoscale_cluster.py`

### Purpose

Wraps `ClusterWorkerPool` and automatically scales Proxmox workers based on workload:

- Monitors local queue (`PriorityWorkerPool`) and inflight tasks
    
- Automatically provisions or tears down Proxmox workers
    
- Adds/removes nodes from `ClusterWorkerPool` dynamically
    
- Works alongside existing remote nodes (HTTP/gRPC) and local pool
    

### Key Classes

- **`ProxmoxAutoClusterPool`** – High-level autoscaling cluster interface.
    

### Example

```python
from worker_pool import PriorityWorkerPool
from proxmox_worker_manager import ProxmoxWorkerManager
from proxmox_autoscale_cluster import ProxmoxAutoClusterPool

# Local worker pool
local_pool = PriorityWorkerPool(worker_count=4)
local_pool.start()

# Proxmox manager
proxmox_mgr = ProxmoxWorkerManager(api_host="pve.local", user="root@pam", password="secret")

# Autoscaling cluster
cluster = ProxmoxAutoClusterPool(
    local_pool=local_pool,
    proxmox_manager=proxmox_mgr,
    max_workers=6,
    min_workers=1,
    node_labels=("exec",)
)

cluster.start_autoscaler()

# Submit a task (automatically routed to local or Proxmox node)
task_id = cluster.submit_task(
    "tools.run_toolchain",
    {"plan": "execute complex workflow"},
    priority=Priority.HIGH,
    labels=("exec",)
)
```

---

## Integration with `worker_pool` and `ClusterWorkerPool`

1. **Local Pool** – Each machine runs a `PriorityWorkerPool` handling CPU-sensitive tasks.
    
2. **Cluster Routing** – `ClusterWorkerPool` can route tasks to:
    
    - Local pool
        
    - Remote HTTP/gRPC workers
        
    - Proxmox workers (via `ProxmoxAutoClusterPool`)
        
3. **Autoscaling** – `ProxmoxAutoClusterPool` monitors queue length and worker load to add/remove VMs dynamically.
    
4. **LangChain Tool Integration** – Any task submitted via the cluster can be wrapped as a LangChain tool, enabling:
    
    - Automatic label-based routing
        
    - Priority handling
        
    - Retry/backoff support
        
    - Multi-host execution
        

---

## Notes

- Each Proxmox worker exposes a lightweight HTTP executor (`worker_pool` compatible) on a defined port.
    
- Rate-limiting, CPU thresholds, and per-label concurrency are still enforced.
    
- Can be combined with other remote nodes to handle light LLM requests, heavy LLMs, or tool execution.
    
- Fully thread-safe and non-blocking for production workloads.
    

---

This setup allows you to build **autonomous, distributed workflows** that:

- Automatically scale Proxmox VMs
    
- Route tasks intelligently across local and remote workers
    
- Integrate seamlessly into LangChain pipelines
    
- Maintain high responsiveness for interactive agents
    

---

## Use Case 1: Local CPU-bound tasks

**Scenario:** You want to run high-priority, resource-sensitive jobs locally without overwhelming the system.

**Implementation:**

- **Module:** `worker_pool.PriorityWorkerPool`
    
- **How it works:**
    
    - Tasks are submitted with priorities (`Priority.CRITICAL`, `Priority.HIGH`, etc.)
        
    - CPU and process usage thresholds prevent overloading local machine
        
    - Rate-limits can be applied per label to prevent starvation
        
- **Example:**
    
    ```python
    local_pool = PriorityWorkerPool(worker_count=4, cpu_threshold=85.0, max_process_name="ollama", max_processes=24)
    local_pool.submit(func=my_task, priority=Priority.HIGH, labels=("llm",))
    ```
    
- **Benefit:** Guarantees responsiveness for low-latency tasks while heavy work runs on separate hosts.
    

---

## Use Case 2: Offloading to remote nodes (non-Proxmox)

**Scenario:** Run tasks on other servers in your network (e.g., lightweight LLM, or heavy computation).

**Implementation:**

- **Modules:** `transport_http`, `cluster.ClusterWorkerPool`
    
- **How it works:**
    
    - `ClusterWorkerPool` maintains local pool + list of remote nodes (`RemoteNode`)
        
    - Labels indicate task capabilities (e.g., `"llm_light"`, `"exec"`)
        
    - Tasks are routed to the least busy node supporting required labels
        
- **Example:**
    
    ```python
    cluster = ClusterWorkerPool(local_pool)
    cluster.add_node(RemoteNode(name="light", base_url="http://192.168.1.20:8081", labels=("llm_light","llm"), auth_token="secret"))
    cluster.submit_task("llm.generate", {"prompt": "Summarize issue #42"}, labels=("llm_light",), priority=Priority.HIGH)
    ```
    
- **Benefit:** Can scale horizontally using existing hardware without provisioning new VMs.
    

---

## Use Case 3: Proxmox-hosted workers

**Scenario:** Dynamically spin up VMs or containers in Proxmox to run tasks.

**Implementation:**

- **Modules:** `proxmox_worker_manager`, `proxmox_autoscale_cluster`
    
- **How it works:**
    
    - `ProxmoxWorkerManager` uses API or SSH to create/manage worker VMs/containers
        
    - Workers run a lightweight executor (HTTP server) exposing `PriorityWorkerPool` interface
        
    - `ProxmoxAutoClusterPool` integrates these nodes into `ClusterWorkerPool` and autoscaling logic
        
- **Example:**
    
    ```python
    proxmox_mgr = ProxmoxWorkerManager(api_host="pve.local", user="root@pam", password="secret")
    cluster = ProxmoxAutoClusterPool(local_pool=local_pool, proxmox_manager=proxmox_mgr, max_workers=5)
    cluster.start_autoscaler()
    cluster.submit_task("tools.run_toolchain", {"plan": "heavy job"}, labels=("exec",), priority=Priority.NORMAL)
    ```
    
- **Benefit:** Auto-scales workers on-demand, reducing idle cost while ensuring enough capacity for spikes in workload.
    

---

## Use Case 4: Multi-tier LLM execution

**Scenario:** Use lightweight nodes for fast responses while heavy nodes do in-depth processing.

**Implementation:**

- **Modules:** `ClusterWorkerPool` + `ProxmoxAutoClusterPool`
    
- **How it works:**
    
    - Lightweight remote nodes handle short queries (`labels: llm_light`)
        
    - Heavy LLM runs on Proxmox or dedicated servers (`labels: llm_heavy`)
        
    - Tasks submitted with appropriate labels are routed automatically
        
- **Example:**
    
    ```python
    cluster.add_node(RemoteNode(name="heavy", base_url="http://heavy-host:8082", labels=("llm_heavy","exec"), auth_token="secret"))
    cluster.submit_task("llm.generate", {"prompt": "Deep RAG search"}, labels=("llm_heavy",))
    ```
    
- **Benefit:** Keeps interactive tasks snappy without blocking heavy computation.
    

---

## Use Case 5: LangChain integration

**Scenario:** Use these workers as tools in a LangChain agent.

**Implementation:**

- **Modules:** All above + LangChain tool wrapper
    
- **How it works:**
    
    - Wrap cluster.submit_task as a LangChain `Tool`
        
    - Labels and priorities allow agent to pick appropriate worker
        
    - Proactive background management (`ProactiveFocusManager`) can also schedule tasks automatically
        
- **Example:**
    
    ```python
    from langchain.agents import Tool
    
    def execute_plan(prompt: str):
        return cluster.submit_task("tools.run_toolchain", {"plan": prompt}, labels=("exec",))
    
    tool = Tool(name="ExecutePlan", func=execute_plan)
    ```
    
- **Benefit:** Agents gain distributed, high-throughput task execution transparently.
    

---

## Use Case 6: Autoscaling Proxmox cluster

**Scenario:** Dynamically adjust number of Proxmox worker VMs based on load.

**Implementation:**

- **Module:** `ProxmoxAutoClusterPool`
    
- **How it works:**
    
    - Monitors `PriorityWorkerPool` queue length and inflight tasks
        
    - Spins up additional workers when backlog grows
        
    - Tears down idle workers to save resources
        
- **Example:**
    
    ```python
    cluster.start_autoscaler(poll_interval=15.0)
    ```
    
- **Benefit:** Automatic horizontal scaling without manual provisioning.
    

---

## Use Case 7: Multi-host redundancy

**Scenario:** Mix Proxmox workers and external servers for reliability.

**Implementation:**

- **Modules:** `ClusterWorkerPool` + `ProxmoxAutoClusterPool` + `RemoteNode`
    
- **How it works:**
    
    - `ClusterWorkerPool` routes tasks to available node with capacity
        
    - If a node fails, task can be retried locally or on another node
        
- **Benefit:** High availability and fault tolerance across mixed infrastructure.
    

---

